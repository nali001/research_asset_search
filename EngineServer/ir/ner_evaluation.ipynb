{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "def read_conll_file(file_path):\n",
    "    \"\"\"Reads a CoNLL format file and returns a list of token-label pairs for each sentence.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    data.append(sentence)\n",
    "                    sentence = []\n",
    "            else:\n",
    "                token, label = line.split('\\t')\n",
    "                sentence.append((token, label))\n",
    "        if sentence:\n",
    "            data.append(sentence)\n",
    "    return data\n",
    "\n",
    "def convert_json_to_conll(system_output_json):\n",
    "    \"\"\"Converts system output from JSON format to CoNLL format.\"\"\"\n",
    "    sentence = [(token, label) for label, token in system_output_json.items()]\n",
    "    return [sentence]\n",
    "\n",
    "def convert_annotation_to_conll(annotation):\n",
    "    \"\"\"Converts annotation from the provided format to CoNLL format.\"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in annotation.strip().split('\\n'):\n",
    "        if line:\n",
    "            parts = line.split()\n",
    "            label = parts[1]\n",
    "            start, end = int(parts[2]), int(parts[3])\n",
    "            token = parts[4]\n",
    "            sentence.append((token, label))\n",
    "        else:\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def compute_metrics(system_output, annotations):\n",
    "    \"\"\"Computes precision, recall, F1 score, and accuracy.\"\"\"\n",
    "    true_positives = sum(1 for s1, s2 in zip(system_output, annotations) for t1, l1 in s1 for t2, l2 in s2 if t1 == t2 and l1 == l2)\n",
    "    predicted_entities = sum(len(s) for s in system_output)\n",
    "    true_entities = sum(len(s) for s in annotations)\n",
    "\n",
    "    precision = true_positives / predicted_entities if predicted_entities > 0 else 0\n",
    "    recall = true_positives / true_entities if true_entities > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    accuracy = true_positives / sum(len(s) for s in annotations) if annotations else 0\n",
    "\n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "# Sample system output in JSON format\n",
    "system_output_json = {\n",
    "    \"task\": \"statistical sentence generation\",\n",
    "    \"dataset\": None,\n",
    "    \"method\": \"packed sets of trees\"\n",
    "}\n",
    "\n",
    "# Sample annotation in the provided format\n",
    "annotation_sample = \"\"\"\n",
    "T1\tGeneric 27 35\tapproach\n",
    "T2\tTask 40 71\tstatistical sentence generation\n",
    "T3\tOtherScientificTerm 139 144\ttrees\n",
    "T4\tOtherScientificTerm 151 158\tforests\n",
    "T5\tGeneric 220 234\trepresentation\n",
    "T6\tOtherScientificTerm 301 322\tsyntactic information\n",
    "T7\tGeneric 325 327\tIt\n",
    "T8\tMethod 361 380\tstatistical ranking\n",
    "T9\tGeneric 398 406\tapproach\n",
    "T10\tTask 411 433\tstatistical generation\n",
    "\"\"\"\n",
    "\n",
    "# Convert the system output JSON to CoNLL format\n",
    "system_output_conll = convert_json_to_conll(system_output_json)\n",
    "\n",
    "# Convert annotation to CoNLL format\n",
    "annotations_conll = convert_annotation_to_conll(annotation_sample)\n",
    "\n",
    "# Compute metrics\n",
    "precision, recall, f1_score, accuracy = compute_metrics(system_output_conll, annotations_conll)\n",
    "\n",
    "# Print the computed metrics\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1_score)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('statistical sentence generation', 'task'),\n",
       "  (None, 'dataset'),\n",
       "  ('packed sets of trees', 'method')]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_output_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('approach', 'Generic'),\n",
       "  ('statistical', 'Task'),\n",
       "  ('trees', 'OtherScientificTerm'),\n",
       "  ('forests', 'OtherScientificTerm'),\n",
       "  ('representation', 'Generic'),\n",
       "  ('syntactic', 'OtherScientificTerm'),\n",
       "  ('It', 'Generic'),\n",
       "  ('statistical', 'Method'),\n",
       "  ('approach', 'Generic'),\n",
       "  ('statistical', 'Task')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooksearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

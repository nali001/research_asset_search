{
    "docid": "Kaggle2614",
    "source_id": "arifali77/top-5-deep-learning-frameworks-tutorial",
    "name": "Top 5 Deep Learning Frameworks Tutorial",
    "file_name": "arifali77_top-5-deep-learning-frameworks-tutorial.ipynb",
    "source": "Kaggle",
    "html_url": "https://www.kaggle.com/code/arifali77/top-5-deep-learning-frameworks-tutorial",
    "description": "# <div style=\"text-align: center\">Top 5 Deep Learning Frameworks Tutorial </div>\n ### <div align=\"center\"><b>CLEAR DATA. MADE MODEL.</b></div>\n <div align=\"center\">Each framework is built in a different manner for different purposes. In this Notebook, we look at the 5 deep learning frameworks to give you a better idea of which framework will be the perfect fit or come handy in solving your **business challenges**.</div>\n <a id=\"top\"></a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n    1. [Courses](#2)\n    1. [Kaggle kernels](#3)\n    1. [Ebooks](#4)\n    1. [Cheat Sheets](#5)\n    1. [Deep Learning vs Machine Learning](#6)\n1. [Loading Packages & Data](#7)\n    1. [Version](#8)\n    1. [Setup](#9)\n    1. [Loading Data](#10)\n        1. [Data fields](#11)\n    1. [EDA](#12)\n1. [Python Deep Learning Packages](#31)\n    1. [Keras](#33)\n        1. [Analysis](#34)\n        1. [Text Classification with Keras](#331)\n    1. [TensorFlow](#35)\n        1. [Import the Fashion MNIST dataset](#36)\n        1. [Explore the data](#37)\n        1. [Preprocess the data](#38)\n        1. [Build the model](#39)\n            1. [Setup the layers](#40)\n        1. [Compile the model](#41)\n        1. [Train the model](#42)\n        1. [Evaluate accuracy](#43)\n        1. [Make predictions](#44)\n    1. [Theano](#45)\n        1. [Theano( example)](#46)\n        1. [Calculating multiple results at once](#47)\n    1. [Pytroch](#48)\n        1. [Tensors](#49)\n        1. [Operations](#50)\n    1. [CNTK](#51)\n1. [Conclusion](#51)\n1. [References](#52)\n <a id=\"1\"></a> <br>\n## 1- Introduction\nThis is a **comprehensive Deep Learning techniques with python**, it is clear that everyone in this community is familiar with **MNIST dataset**  and other data sets that I want to use in this kernel sach as **Quora** but if you need to review your information about the dataset please visit [MNIST](https://en.wikipedia.org/wiki/MNIST_database), [Quora](https://www.kaggle.com/c/quora-insincere-questions-classification).\n\nI have tried to help  Kaggle users  how to face deep learning problems. and I think it is a great opportunity for who want to learn deep learning workflow with python completely.\n<a id=\"2\"></a> <br>\n## 1-1 Courses\nThere are a lot of online courses that can help you develop your knowledge, here I have just  listed some of them that I have used in my Kernel:\n\n1. [Deep Learning Certification by Andrew Ng from deeplearning.ai (Coursera)](https://www.coursera.org/specializations/deep-learning)\n1. [Deep Learning A-Z™: Hands-On Artificial Neural Networks](https://www.udemy.com/deeplearning/)\n\n1. [Creative Applications of Deep Learning with TensorFlow](https://www.class-central.com/course/kadenze-creative-applications-of-deep-learning-with-tensorflow-6679)\n1. [Neural Networks for Machine Learning](https://www.class-central.com/mooc/398/coursera-neural-networks-for-machine-learning)\n1. [Practical Deep Learning For Coders, Part 1](https://www.class-central.com/mooc/7887/practical-deep-learning-for-coders-part-1)\n1. [cs231n](http://cs231n.stanford.edu/)\n<a id=\"3\"></a> <br>\n\n## 1-2 Kaggle kernels\nI want to thanks **Kaggle team**  and  all of the **kernel's authors**  who develop this huge resources for Data scientists. I have learned from The work of others and I have just listed some more important kernels that inspired my work and I've used them in this kernel:\n\n1. [Deep Learning Tutorial for Beginners](https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners)\n1. [introduction-to-cnn-keras-0-997-top-6](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)\n\n<a id=\"4\"></a> <br>\n## 1-3 Ebooks\nSo you love reading , here is **10 free machine learning books**\n1. [Probability and Statistics for Programmers](http://www.greenteapress.com/thinkstats/)\n2. [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/091117.pdf)\n2. [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n2. [Understanding Machine Learning](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html)\n2. [A Programmer’s Guide to Data Mining](http://guidetodatamining.com/)\n2. [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf)\n2. [A Brief Introduction to Neural Networks](http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf)\n2. [Deep Learning](http://www.deeplearningbook.org/)\n2. [Natural Language Processing with Python](https://www.researchgate.net/publication/220691633_Natural_Language_Processing_with_Python)\n2. [Machine Learning Yearning](http://www.mlyearning.org/)\n<a id=\"5\"></a> <br>\n\n## 1-4 Cheat Sheets\nData Science is an ever-growing field, there are numerous tools & techniques to remember. It is not possible for anyone to remember all the functions, operations and formulas of each concept. That’s why we have cheat sheets. But there are a plethora of cheat sheets available out there, choosing the right cheat sheet is a tough task. So, I decided to write this article.\n\nHere I have selected the cheat sheets on the following criteria: comprehensiveness, clarity, and content [26]:\n1. [Quick Guide to learn Python for Data Science ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Data-Science-in-Python.pdf)\n1. [Python for Data Science Cheat sheet ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/beginners_python_cheat_sheet.pdf)\n1. [Python For Data Science Cheat Sheet NumPy](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Numpy_Python_Cheat_Sheet.pdf)\n1. [Exploratory Data Analysis in Python]()\n1. [Data Exploration using Pandas in Python](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Data-Exploration-in-Python.pdf)\n1. [Data Visualisation in Python](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/data-visualisation-infographics1.jpg)\n1. [Python For Data Science Cheat Sheet Bokeh](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Python_Bokeh_Cheat_Sheet.pdf)\n1. [Cheat Sheet: Scikit Learn ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Scikit-Learn-Infographic.pdf)\n1. [MLalgorithms CheatSheet](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/MLalgorithms-.pdf)\n1. [Probability Basics  Cheat Sheet ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/probability_cheatsheet.pdf)\n<a id=\"6\"></a> <br>\n\n## 1-5 Deep Learning vs Machine Learning\nWe use a **machine algorithm** to parse data, learn from that data, and make informed decisions based on what it has learned. Basically, **Deep Learning** is used in layers to create an **Artificial Neural Network** that can learn and make intelligent decisions on its own. We can say **Deep Learning is a sub-field of Machine Learning**.\n\n<img src =\"http://blog.thinkwik.com/wp-content/uploads/2018/07/Insights-of-The-Machine-Learning-and-The-Deep-Learning.png\">\n[image Credits](http://blog.thinkwik.com/insights-of-the-machine-learning-and-the-deep-learning/)\n\nI am open to getting your feedback for improving this **kernel**\n\n###### [Go to top](#top)\n<a id=\"7\"></a> <br>\n# 2 Loading Packages & Data\nIn this kernel we are using the following packages:\n<a id=\"8\"></a> <br>\n## 2-1 Version\nPrint version of each package.\n###### [Go to top](#top)\n<a id=\"9\"></a> <br>\n## 2-2 Setup\n\nA few tiny adjustments for better **code readability**\n<a id=\"10\"></a> <br>\n## 2-3 Loading Data\nData collection is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia](https://www.techopedia.com/definition/30318/data-collection) I start Collection Data by the training and testing datasets into Pandas DataFrames. Each row is an observation (also known as : sample, example, instance, record) Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate).\n### 2-3-1 What is a insincere question?\nis defined as a question intended to make a **statement** rather than look for helpful **answers**. \n### 2-3-2 how can we find  insincere question?\nSome **characteristics** that can signify that a question is **insincere**:\n\n1. **Has a non-neutral tone**\n    1. Has an exaggerated tone to underscore a point about a group of people\n    1. Is rhetorical and meant to imply a statement about a group of people\n1. **Is disparaging or inflammatory**\n    1. Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n    1. Makes disparaging attacks/insults against a specific person or group of people\n    1. Based on an outlandish premise about a group of people\n    1. Disparages against a characteristic that is not fixable and not measurable\n1. **Isn't grounded in reality**\n    1. Based on false information, or contains absurd assumptions\n1. **Uses sexual content** (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers\n\nAfter loading the data via pandas, we should checkout what the content is, description and via the following:\n###### [Go to top](#top)\n<a id=\"11\"></a> <br>\n### 2-3-1 Data fields\n1. qid - unique question identifier\n1. question_text - Quora question text\n1. target - a question labeled \"insincere\" has a value of 1, otherwise 0\nTo check the first 5 rows of the data set, we can use head(5).\n<a id=\"11\"></a> <br>\n### 2-3-2 Target\nYou will be predicting whether a question asked on **Quora** is sincere or not.\n<a id=\"12\"></a> <br>\n## 2-4 EDA\nIn this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data.\n1. Which variables suggest interesting relationships?\n1. Which observations are unusual?\n1. Analysis of the features! By the end of the section, you'll be able to answer these questions and more, while generating. \nTo pop up 5 random rows from the data set, we can use **sample(5**) function.\nTo check out last 5 row of the data set, we use tail() function.\nTo check out how many null info are on the dataset, we can use **isnull().sum()**.\n### 2-4-1 About Quora\nQuora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\nto give a statistical summary about the dataset, we can use **describe()**.\n###### [Go to top](#top)\n<a id=\"31\"></a> <br>\n# 3- Python Deep Learning Packages\n<img src='https://cdn-images-1.medium.com/max/800/1*dYjDEI0mLpsCOySKUuX1VA.png'>\n*State of open source deep learning frameworks in 2017* [**towardsdatascience**](https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750)\n1. **keras**[11]\n>Well known for being minimalistic, the Keras neural network library (with a supporting interface of Python) supports both convolutional and recurrent networks that are capable of running on either TensorFlow or Theano. The library is written in Python and was developed keeping quick experimentation as its USP.\n1. **TensorFlow**\n> TensorFlow is arguably one of the best deep learning frameworks and has been adopted by several giants such as Airbus, Twitter, IBM, and others mainly due to its highly flexible system architecture.\n1. **Caffe**\n> Caffe is a deep learning framework that is supported with interfaces like C, C++, Python, and MATLAB as well as the command line interface. It is well known for its speed and transposability and its applicability in modeling convolution neural networks (CNN).\n1. **Microsoft Cognitive Toolkit/CNTK**\n> Popularly known for easy training and the combination of popular model types across servers, the Microsoft Cognitive Toolkit (previously known as CNTK) is an open-source deep learning framework to train deep learning models. It performs efficient convolution neural networks and training for image, speech, and text-based data. Similar to Caffe, it is supported by interfaces such as Python, C++, and the command line interface.\n1. **Torch/PyTorch**\n> Torch is a scientific computing framework that offers wide support for machine learning algorithms. It is a Lua-based deep learning framework and is used widely amongst industry giants such as Facebook, Twitter, and Google. It employs CUDA along with C/C++ libraries for processing and was basically made to scale the production of building models and provide overall flexibility.\n1. **MXNet**\n> Designed specifically for the purpose of high efficiency, productivity, and flexibility, MXNet(pronounced as mix-net) is a deep learning framework supported by Python, R, C++, and Julia.\n1. **Chainer**\n>Highly powerful, dynamic and intuitive, Chainer is a Python-based deep learning framework for neural networks that is designed by the run strategy. Compared to other frameworks that use the same strategy, you can modify the networks during runtime, allowing you to execute arbitrary control flow statements.\n1. **Deeplearning4j**\n>Parallel training through iterative reduce, microservice architecture adaptation, and distributed CPUs and GPUs are some of the salient features of the Deeplearning4j deep learning framework. It is developed in Java as well as Scala and supports other JVM languages, too.\n1. **Theano**\n>Theano is beautiful. Without Theano, we wouldn’t have anywhere near the amount of deep learning libraries (specifically in Python) that we do today. In the same way that without NumPy, we couldn’t have SciPy, scikit-learn, and scikit-image, the same can be said about Theano and higher-level abstractions of deep learning.\n1. **Lasagne**\n>Lasagne is a lightweight library used to construct and train networks in Theano. The key term here is lightweight — it is not meant to be a heavy wrapper around Theano like Keras is. While this leads to your code being more verbose, it does free you from any restraints, while still giving you modular building blocks based on Theano.\n1. **PaddlePaddle**\n>PaddlePaddle (PArallel Distributed Deep LEarning) is an easy-to-use, efficient, flexible and scalable deep learning platform, which is originally developed by Baidu scientists and engineers for the purpose of applying deep learning to many products at Baidu.\n\n###### [Go to top](#top)\n<a id=\"32\"></a> <br>\n# 4- Frameworks\nLet's Start Learning, in this section we intrduce 5 deep learning frameworks.\n<a id=\"33\"></a> <br>\n## 4-1 Keras\nOur workflow will be as follow[10] [deep-learning-with-python-notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks):\n1. first we will present our neural network with the training data, `train_images` and `train_labels`. \n1. The network will then learn to associate images and labels. \n1. Finally, we will ask the network to produce predictions for `test_images`, \n1. and we  will verify if these predictions match the labels from `test_labels`.\n\n**Let's build our network **\n\n###### [Go to top](#top)\n<a id=\"34\"></a> <br>\n## 4-1-1 Analysis\nThe core building block of neural networks is the \"**layer**\", a data-processing module which you can conceive as a \"**filter**\" for data. Some  data comes in, and comes out in a more useful form. Precisely, layers extract _representations_ out of the data fed into them -- hopefully  representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers which will implement a form of progressive \"**data distillation**\". [colab.research.google](https://colab.research.google.com/github/alzayats/Google_Colab/blob/master/2_1_a_first_look_at_a_neural_network.ipynb)\nA deep learning model is like a sieve for data processing, made of a succession of increasingly refined data filters -- the \"layers\".\nHere our network consists of a sequence of two `Dense` layers, which are densely-connected (also called \"fully-connected\") neural layers. \nThe second (and last) layer is a 10-way \"**softmax**\" layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\nTo make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n\n1. A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be able to steer itself in the right direction.\n1. An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n1. Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly classified).\n\n###### [Go to top](#top)\n\nBefore training, we will **preprocess** our data by reshaping it into the shape that the network expects, and **scaling** it so that all values are in \nthe `[0, 1]` interval. Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with \nvalues in the `[0, 255]` interval. We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1.\n\n###### [Go to top](#top)\nWe also need to **categorically encode** the labels\nWe are now ready to train our network, which in **Keras** is done via a call to the `fit` method of the network: \nwe \"fit\" the model to its training data.\n\n###### [Go to top](#top)\n**Two quantities** are being displayed during training: the \"**loss**\" of the network over the training data, and the accuracy of the network over \nthe training data.\n\nWe quickly reach an accuracy of **0.989 (i.e. 98.9%)** on the training data. Now let's check that our model performs well on the test set too:\n\n###### [Go to top](#top)\n\n**Our test set accuracy turns out to be 97.8%**\n<a id=\"331\"></a> <br>\n## 4-1-2  Text Classification with Keras\nA simple text classification from [16].\n<a id=\"35\"></a> <br>\n## 4-2 TensorFlow\n**TensorFlow** is an open-source machine learning library for research and production. TensorFlow offers **APIs** for beginners and experts to develop for desktop, mobile, web, and cloud. See the sections below to get started.[12] [tensorflow](https://www.tensorflow.org/tutorials)\n\n###### [Go to top](#top)\n<a id=\"36\"></a> <br>\n## 4-2-1 Import the Fashion MNIST dataset\n\nLoading the dataset returns **four NumPy arrays**:\n\n1. The train_images and train_labels arrays are the training set—the data the model uses to learn.\n1. The model is tested against the test set, the test_images, and test_labels arrays.\n1. The images are 28x28 NumPy arrays, with pixel values ranging between 0 and 255.\n1. The labels are an array of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\n###### [Go to top](#top)\n<img src='https://tensorflow.org/images/fashion-mnist-sprite.png'>\n[image credit](https://tensorflow.org)\nEach image is **mapped** to a single label. Since the class names are not included with the dataset, store them here to use later when plotting the images:\n<a id=\"37\"></a> <br>\n## 4-2-2 Explore the data\nLet's explore the format of the dataset before training the model. The following shows there are **60,000** images in the training set, with each image represented as 28 x 28 pixels:\n\n###### [Go to top](#top)\nLikewise, there are 60,000 labels in the training set:\n\n\nEach label is an integer between 0 and 9:\n\n\nThere are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\n\nAnd the test set contains 10,000 images labels:\n\n\n<a id=\"38\"></a> <br>\n## 4-2-3 Preprocess the data\n\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\n###### [Go to top](#top)\n\nWe scale these values to a range of 0 to 1 before feeding to the neural network model. For this, cast the datatype of the image components from an** integer to a float,** and divide by 255. Here's the function to preprocess the images:\n\nIt's important that the training set and the testing set are preprocessed in the same way:\nDisplay the first 25 images from the training set and display the class name below each image. **Verify** that the data is in the correct format and we're ready to build and train the network.\n<a id=\"39\"></a> <br>\n## 4-2-4 Build the model\n\n**Building the neural network requires configuring the layers of the model, then compiling the model.**\n<a id=\"40\"></a> <br>\n### 4-2-4-1 Setup the layers\nThe basic building block of a neural network is the layer. **Layers** extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\n\nMost of deep learning consists of chaining together simple layers. Most layers, like tf.keras.layers.Dense, have parameters that are learned during training.\n\n###### [Go to top](#top)\nThe **first layer** in this network, tf.keras.layers.Flatten, transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n\nAfter the pixels are flattened, the network consists of a sequence of two tf.keras.layers.Dense layers. These are densely-connected, or fully-connected, neural layers. The first Dense layer has 128 nodes (or neurons). **The second (and last) layer** is a 10-node softmax layer—this returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 classes.\n\n###### [Go to top](#top)\n<a id=\"41\"></a> <br>\n## 4-2-5 Compile the model\nBefore the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n\n1. **Loss function** —This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n1. **Optimizer** —This is how the model is updated based on the data it sees and its loss function.\n1. **Metrics** —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n\n###### [Go to top](#top)\n<a id=\"42\"></a> <br>\n## 4-2-6 Train the model\nTraining the neural network model requires the following steps:\n\nFeed the training data to the model—in this example, the train_images and train_labels arrays.\nThe model learns to associate images and labels.\nWe ask the model to make predictions about a test set—in this example, the test_images array. We verify that the predictions match the labels from the test_labels array.\nTo start training, call the model.fit method—the model is \"fit\" to the training data:\n\n###### [Go to top](#top)\nAs the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.88 (or 88%) on the training data.\n<a id=\"43\"></a> <br>\n## 4-2-7 Evaluate accuracy\nNext, compare how the model performs on the test dataset:\n\n###### [Go to top](#top)\nIt turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on their training data.\n<a id=\"44\"></a> <br>\n## 4-2-8 Make predictions\nWith the model trained, we can use it to make predictions about some images.\n###### [Go to top](#top)\nHere, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:\n\n\nA prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:\n\n\nWe can graph this to look at the full set of 10 channels\n\n\nLet's look at the 0th image, predictions, and prediction array.\n\n###### [Go to top](#top)\nLet's plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percent (out of 100) for the predicted label. Note that it can be wrong even when very confident.\n\n###### [Go to top](#top)\nFinally, use the trained model to make a **prediction** about a single image.\n\n###### [Go to top](#top)\n**tf.keras** models are optimized to make predictions on a batch, or collection, of examples at once. So even though we're using a single image, we need to add it to a list:\n\n\nNow predict the image:\n\n###### [Go to top](#top)\n<a id=\"45\"></a> <br>\n# 4-3 Theano \n**Theano** is a numerical computation library for Python. It is a common choice for implementing neural network models as it allows you to efficiently define, optimize and evaluate mathematical expressions, including multi-dimensional arrays (numpy.ndaray).[13] [ Credits to journaldev](https://www.journaldev.com/17840/theano-python-tutorial)\n\n###### [Go to top](#top)\nTheano has got an amazing compiler which can do various optimizations of varying complexity. A few of such optimizations are:\n\n1. Arithmetic simplification (e.g: --x -> x; x + y - x -> y)\n1. Using memory aliasing to avoid calculation\n1. Constant folding\n1. Merging similar subgraphs, to avoid redundant calculation\n1. Loop fusion for elementwise sub-expressions\n1. GPU computations\n<a id=\"46\"></a> <br>\n## 4-3-1 Theano( example)\nLet’s have a look at rather more elaborate example than just adding two numbers. Let’s try to compute the **logistic** curve, which is given by:\n<img src='https://cdn.journaldev.com/wp-content/uploads/2018/01/logistic-curve.png'>\nIf we plot a graph for this equation, it will look like:\n\n<img src='https://cdn.journaldev.com/wp-content/uploads/2018/01/logistic-curve-1.png'>\nLogistic function is applied to each element of matrix. Let’s write a code snippet to demonstrate this:\n\n###### [Go to top](#top)\n<a id=\"47\"></a> <br>\n## 4-3-2 Calculating multiple results at once\nLet’s say we have to compute elementwise difference, absolute difference and difference squared between two matrices ‘x’ and ‘y’. Doing this at same time optimizes program with significant duration as we don’t have to go to each element again and again for each operation.\n\n###### [Go to top](#top)\n<a id=\"48\"></a> <br>\n## 4-4 Pytroch\nIt’s a **Python-based** scientific computing package targeted at two sets of audiences:[Credits to pytorch-dynamic-computational](https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1)\n\n1. A replacement for NumPy to use the power of GPUs.\n1. A deep learning research platform that provides maximum flexibility and speed.\n<img src='https://cdn-images-1.medium.com/max/800/1*5PLIVNA5fIqEC8-kZ260KQ.gif'>\n*PyTorch dynamic computational graph — source: http://pytorch.org/about/*\n\n###### [Go to top](#top)\n<a id=\"49\"></a> <br>\n## 4-4-1 Tensors\n**Tensors** are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\nConstruct a 5x3 matrix, uninitialized:\nConstruct a randomly initialized matrix:\n\n\nConstruct a matrix filled zeros and of dtype long:\n\n###### [Go to top](#top)\nConstruct a tensor directly from data:\n\n\nOr create a tensor based on an existing tensor. These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by user.\n\n###### [Go to top](#top)\nGet its size:\n<a id=\"50\"></a> <br>\n## 4-4-2 Operations\nThere are multiple syntaxes for operations. In the following example, we will take a look at the addition operation.\n\nAddition: syntax 1.\n\n###### [Go to top](#top)\nAddition: syntax 2\n\n\nAddition: providing an output **tensor** as argument.\n\n\n<a id=\"52\"></a> <br>\n# 5- Conclusion\nIn this kernel we have just tried to create a **comprehensive deep learning workflow** for helping you to  start your jounery in DL.\nsurly it is not **completed yet**!! also I want to hear your voice to improve kernel together.\n",
    "summarization": "the 5 deep learning frameworks are designed to help you solve your business challenges. we look at the top 5 'deep learning Frameworks' to give you a better idea of which framework will be the perfect fit for your needs",
    "summarization_relevance": "0.62",
    "summarization_confidence": "0.83",
    "language": "python",
    "num_cells": "145"
}
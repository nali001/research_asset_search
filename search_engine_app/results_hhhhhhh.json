{"query": "cell segmentation ", "facets": [], "num_hits": 161, "num_pages": 11, "current_page": 1, "results": [{"source_id": "ayuraj/hpa-segmentation-mask-visualization-with-w-b", "name": "HPA: Segmentation Mask Visualization with W&B", "file_name": "ayuraj_hpa-segmentation-mask-visualization-with-w-b.ipynb", "html_url": "https://www.kaggle.com/code/ayuraj/hpa-segmentation-mask-visualization-with-w-b", "description": "In this notebook I will be using [Weights and Biases](https://wandb.ai/site) overlay visualization tool to interactively visualize all the segmentation mask generated using the provided HPASegmentation tool.\n\nHere's the W&B report summarizing the results: http://bit.ly/play-with-segmentation-masks\n![](https://i.imgur.com/9kNLI4L.gif)\nIn this competition, we are provided with image-level labels, and the task is to classify each cell in a given image into one or multiple labels.\n\n* Thus, each image has multiple numbers of cells. \n\n* Each cell consists of multiple [organelles](https://www.genome.gov/genetics-glossary/Organelle). In the previous [HPA competition](https://www.kaggle.com/c/human-protein-atlas-image-classification), 28 organelles were used as labels, and the task was to predict image-level labels (given input image predict the organelles). \n\n* In this competition, the task is to predict cell-level labels using signals from image-level labels. That's what makes it a more challenging problem statement. \n\n* **But what are we predicting?** There's a specific _protein of interest_(signal in the green channel) that can be present in an organelle or multiple organelles in each cell. The image-level labels point to the presence of that protein in the cells *in general*. Thus at the cell-level, \nthe protein might not be present in the ground truth organelle. Interesting!\n\n* This calls for cell segmentation. We have to know the presence of the cells in an image. But we also need to differentiate one cell from another cell. Thus it's instance segmentation. \n\n* The authors have provided with [HPA-Cell-Segmentation](https://github.com/CellProfiling/HPA-Cell-Segmentation) tool. **Is it good?** In a discussion thread, I read that it can accurately segment cells in ~90% of test set images. That's an excellent baseline to start with and focus on cell-level classification. \n# Imports and Setups\n### W&B Setup\n\n* Install wandb. \u2714\ufe0f\n* Create an account on https://wandb.ai (it's free)\n* Input your personal authentication token key. You can get your auth key [here](https://wandb.ai/authorize).\n### (Hyper)parameters\n# Prepare Dataset\n### Get `train.csv`\n### Path to images\n* We will not be needing the green channel to get the segmentation mask as shown in [this kernel](https://www.kaggle.com/lnhtrang/hpa-public-data-download-and-hpacellseg).\n\n* There are a total of 21,806 training images. We will visualize a small fraction of the images. Feel free to use Weights and Biases overlay tool to visualize more images.\n# Initialize Segmentation Model\n# Log Segmentation Masks\nAfter the above cell is executed head over to the W&B project to visualize the segmentation masks and other visualizations. \n\n**Note**: Since we have silenced the W&B logs head over to your W&B profile page and open `hpa-segmentation-mask` project.\n\nHere's the link to my W&B project: https://wandb.ai/ayush-thakur/hpa-segmentation-mask\n\n![](https://i.imgur.com/iw3rETK.png)\n", "source": "Kaggle", "docid": "Kaggle593", "language": "python", "num_cells": 24, "num_code_cells": 11, "num_md_cells": 13, "len_md_text": 2960, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "in this competition, we are provided with image-level labels. the task is to classify each cell in a given image into one or multiple labels. each image has multiple numbers of cells."}, {"source_id": "samusram/hpa-classifier-explainability-segmentation", "name": "HPA - classifier + explainability + segmentation", "file_name": "samusram_hpa-classifier-explainability-segmentation.ipynb", "html_url": "https://www.kaggle.com/code/samusram/hpa-classifier-explainability-segmentation", "description": "# Intro\nIn this notebook, I'd create a simple baseline. I'll build a classifier on top of image-level labels (multi-label classification), then use an explainability technique Guided-GRADCAM to extract regions responsible for particular class prediction, and then assign the segmented cells to particular classes based on the overlap with Grad-CAM outputs.\n\n**Credits**:\nTo segment cells offline, I'll use [this notebook by RDizzl3](https://www.kaggle.com/rdizzl3/hpa-segmentation-masks-no-internet), the corresponding datasets. I also checked out the batched version from [this notebook by Darek K\u0142eczek](https://www.kaggle.com/thedrcat/hpa-baseline-cell-segmentation).\n\n# Plan\n1. [Libraries](#Libraries)\n2. [Data Generators](#Data-Generators)\n  * [One-hot encoding classes](#One-hot-encoding-classes)\n  * [Stratified split into train/val](#Stratified-split-into-train/val)\n  * [Generator class](#Generator-class)\n3. [PR-AUC-based Callback](#PR-AUC-based-Callback)\n4. [Classifier](#Classifier)\n  * [Defining a model](#Defining-a-model)\n  * [Initial tuning of the added fully-connected layer](#Initial-tuning-of-the-added-fully-connected-layer)\n  * [Training the whole model](#Training-the-whole-model)\n  * [Visualizing train and val PR AUC](#Visualizing-train-and-val-PR-AUC)\n5. [Extracting Integrated gradients](#Extracting-Integrated-gradients)\n6. [Cell segmentation](#Cell-segmentation)\n7. [Cell level predictions](#Cell-level-predictions)\n8. [Submission](#Submission)\n# Libraries\n# Data Generators\n## One-hot encoding classes\n## Stratified split into train/val\nLet's stratify based on combination of labels. The unique combinations will be put into train.\n## Generator class\nUsing green filter, as \"the green filter should be used to predict the label, and the other filters are used as references.\" ([from the data page](https://www.kaggle.com/c/hpa-single-cell-image-classification/data))\nGenerator instances\n# PR-AUC-based Callback\nThe callback would be used:\n1. to estimate AUC under precision recall curve for each class,\n2. to early stop after 5 epochs of no improvement in mean PR AUC,\n3. save a model with the best PR AUC in validation,\n4. to reduce learning rate on PR AUC plateau.\nCallback instances\n# Classifier\n## Defining a model\n## Initial tuning of the added fully-connected layer\n## Training the whole model\n## Visualizing train and val PR AUC\nI left the model to train longer on my local GPU. I then upload the best model and plots from the model training.\n# Extracting Integrated gradients\nUsing the awesome tf-explain library.\n# Cell segmentation\n# Cell-level predictions\nLet's use the following heuristics:\n\nwe'll use the 95th percentile of Integrated Gradients output to estimate value roughly corresponding to the image-level confidence. The image conf. level is the predicted class confidence from our classifier.\u00a0\n\nNext, we'll focus on Integrated Gradients output on top of the cell region. As not the whole true-positive cell must be lightened up by Integrated Gradients, we'll use the 95th percentile of the cell's to compare it with the value corresponding to the global image conf. level. Using these two levels we'll estimate the cell-level confidence: the higher the Integrated Gradients output, the higher the confidence.\u00a0\n\nFinally, we'll output only cells with confidence values reaching at least 60% of the image-level confidence.\n# Submission\n", "source": "Kaggle", "docid": "Kaggle407", "language": "python", "num_cells": 62, "num_code_cells": 38, "num_md_cells": 24, "len_md_text": 3358, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "I'll build a classifier on top of image-level labels (multi-label classification), then use an explainability technique Guided-GRADCAM to extract regions responsible for particular class prediction. then assign the segmente"}, {"source_id": "manikyab/cell-segmentation-using-3-mask-r-cnns-and-resnet18", "name": "Cell Segmentation using 3 Mask R-CNNs and Resnet18", "file_name": "manikyab_cell-segmentation-using-3-mask-r-cnns-and-resnet18.ipynb", "html_url": "https://www.kaggle.com/code/manikyab/cell-segmentation-using-3-mask-r-cnns-and-resnet18", "description": "# Cell Segmentation using 3 Mask R-CNNs and Resnet18\nHere, the approach we have taken is to use a Mask R-CNN model (of the state-of-the-art models for instance segmentation), which is based on Faster R-CNNs, to perform instance segmentation. This model, developed by the Facebook AI Research team, is based on a *instance first* strategy instead of *segmentation first* which has been done in other similar models, and has outperformed other approaches on instance segmentation task. \n\nThe R-CNN paper can be found [here](https://arxiv.org/pdf/1703.06870.pdf).\n\nIt has mainly two phases:\n* Region Proposal Network: Here multiple Regions-of-Interest (RoI) are generated by the models.\n* Predicting class, box, and masks: From each RoI, features are extracted which are used to make the predictions.\n \nThe approach we take here is to first use a Resnet18 Classification model to predict the `cell_type`. We also train 3 Mask R-CNNs for the different `cell_type`s and based on the previous prediction, use one of them to get the masks.\n\nThe main intuition behind this is the fact that the shape and number of instances for each type is different, and an improvement is shown with this approach getting a score of 0.283 over the previous 0.275 (while using just a single Mask R-CNN).\n# Imports\n## Configuration\n# Loading the Classification model\n\nWe load a `fastai` learner for a resnet18 model trained to classify the cell type present in the images. This is then used later in the pipeline along with multiple R-CNNs to make mask predictions.\n## Utilities\n\n\n### Transformations\n\nSome of the transformations have been referred from [mask r cnn utils](https://www.kaggle.com/abhishek/maskrcnn-utils) as it was not possible to direclty use the implementation for the transformaions from `torchvision`. The Instance Segmentation task here requires us to also transform the target bounding boxes along with the image, and thus custom transformations are required.\n\nHere, all the transformations all take in `target` as well.\nWe referred to [this mask r cnn utils](https://www.kaggle.com/abhishek/maskrcnn-utils?select=transforms.py) package for the transformations.\n## Training Dataset and DataLoader\nFor defining the custom dataset, `torchvision`'s tutorial notebook (available [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#)) for the PennFudan dataset was helpful.\n# Train loop\n## Model\nThe model has multiple heads for predicting the bounding boxes, classification, and instance masks.\n## Training\n# Analyze prediction results for train set\n## References and Important Links\n\nFor creating and working with the R-CNN model, `torchvision`'s object detection tutorial notebook (available [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#)) was particularly helpful.  \nFor working with instance segmentation task, we referred to an entry from a previous kaggle challenge on segmenting fashion images (available [here](https://www.kaggle.com/abhishek/mask-rcnn-using-torchvision-0-17/notebook)).  \nWe also used pytorch utils for visualizing the masks and understanding the R-CNN output. [Check them here](https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html#instance-seg-output).  \nSome helper functions and the Mask R-CNN model were referred from [this](https://www.kaggle.com/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-273) notebook as well.  \nThe various reference scripts ([avaiable here](https://github.com/pytorch/vision/tree/main/references/detection)) in torchvision were also helpful.\n\nThe Mask R-CNN model was first introduced in [this](https://arxiv.org/abs/1703.06870) paper.\nModel used for classification was trained in [this notebook](https://www.kaggle.com/manikyab/cell-classification-helper-notebook). Interpretation from this notebook has also been done and can be found [here](https://www.kaggle.com/manikyab/cell-classification-captum-interpretation).\n\nThe weights for the model trained can be found [here](https://www.kaggle.com/manikyab/r-cnn-models-for-cell-segmentation).\n", "source": "Kaggle", "docid": "Kaggle520", "language": "python", "num_cells": 71, "num_code_cells": 55, "num_md_cells": 16, "len_md_text": 4061, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "the approach we take here is to first use a Resnet18 Classification model to predict the cell_type. this model is based on Faster R-CNNs, which is used to perform instance segment"}, {"source_id": "enjoyu/exploring-the-human-protein-atlas-images", "name": "Exploring the human protein atlas images", "file_name": "enjoyu_exploring-the-human-protein-atlas-images.ipynb", "html_url": "https://www.kaggle.com/code/enjoyu/exploring-the-human-protein-atlas-images", "description": "# Exploratory data analysis of the human protein atlas image dataset\nupdate 5/10/2018: beginning of cell segmentation algorithm\n\nupdate 5/10/2018: add red + blue channels stack and whole cell identification (does not give a clean result, though)\n\nThis kernel is just the beginning of a work in progress and will be updated very often.\nWe will explore the dataset available for the human protein atlas image competition. Questions we would like to answer include:\n* what channels of the image contain the relevant information\n* how much can we reduce dimensionality of data while retaining important information\n## What's in the data?\n Let's import the *train.csv* data files to see what they contain. We also define a dictionary containing the map between labels of the training data (the column *target* in *train.csv*) and their biological meaning.\nEach image is a 4-channel image with the protein of interest in the green channel. It is the subcellular localization of this protein which is recorded in the *Target* column of the *train.csv* file.\n\nThe red channel corresponds to microtubules,\n\nthe blue channel to the nucleus\n\nand the yellow channel to the endoplasmid reticulum.\n\nLet's display the different channels of the image with ID == 1, since it contains several subcelullar locations for our protein of interest. Then we will overlay the green and yellow channel, as the yellow channel gives a good indication of the cell shape.\nLet's see how the targets are distributed.\nAccording to [Chen *et al*. 2007](https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btm206), if images are segmented into single cell regions, additional features that are not appropriate for whole fields can be calculated after *seeded watershed segmentation*. Nucleus images provide a means to identify each cell, so image segmentation may start by identification of nuclei in images.  The function `cv2.connectedComponents` provides a simple and effective means to label nuclei in images. Conversely, as shown on the following notebook cell, identification of whole cells using `cv2.connectedComponents` is not as efficient, due to the less homogeneous signal in the yellow channel of the image.\nnp.zeros(x): \ud06c\uae30\uac00 \uc815\ud574\uc838 \uc788\uace0 \ubaa8\ub4e0 \uac12\uc774 0\uc778 \ubc30\uc5f4(x:\ubc30\uc5f4\uc758 \ud06c\uae30)\nnp.ones(x): (0\uc774\uc544\ub2cc) 1\ub85c \ucd08\uae30\ud654\ub41c \ubc30\uc5f4\nthreshold: \uc774\ubbf8\uc9c0\ub97c \ud751\uacfc \ubc31\uc73c\ub85c \ub098\ub204\ub294 \uac83. (\uc5ed\uce58\uac12\ubcf4\ub2e4 \ud06c\uba74 \ud53d\uc140\uc138\uae30\uc758 \ucd5c\ub313\uac12\uc73c\ub85c \uc791\uc73c\uba74 0\uc73c\ub85c \ubcc0\ud658)\nmorphological transformation(morphologyEx)\n\n1. erosion: \uc8fc\ubcc0\uc5d0 \uc544\ubb34\uac83\ub3c4 \uc5c6\uc73c\uba74 \ud53d\uc140 \uc81c\uac70.. \uc791\uc740 object \uc81c\uac70\uc5d0 \ud6a8\uacfc\uc801\n\n2. dilation: \uc8fc\ubcc0\uc5d0 \ubb50\uac00 \uc788\uc73c\uba74 \ud53d\uc140 \ucd94\uac00.. \uacbd\uacc4\uac00 \ubd80\ub4dc\ub7ec\uc6cc\uc9c0\uace0 \uad6c\uba4d\uc774 \uba54\uafd4\uc9c0\ub294 \ud6a8\uacfc\n\n3. opening: erosion \uc801\uc6a9 \ud6c4 dilation \uc801\uc6a9. \uc791\uc740 object\ub098 \ub3cc\uae30 \uc81c\uac70\uc5d0 \uc801\ud569\n\n4. closing: dilation \uc801\uc6a9 \ud6c4 erosion \uc801\uc6a9. \uc804\uccb4\uc801\uc778 \uc724\uacfd \ud30c\uc545\uc5d0 \uc801\ud569\nLet's try different simple thresholding methods. Description of threshold types can be found [here](https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html) and [here](https://docs.opencv.org/3.4/d7/d1b/group__imgproc__misc.html#gaa9e58d2860d4afa658ef70a9b1115576).\n*To zero* simple thresholding is not adapted at all for identifying cell boundaries based on the yellow channel. Even after playing with the upper and lower parameter values, no satisfactory result is obtained. *Binary* and *truncate* methods work better. Let's see how *connectedComponents* work after both thresholding methods.\ntruncate: \ud53d\uc140 \uac12\uc774 \uc5ed\uce58\uac12\ubcf4\ub2e4 \ud06c\uba74 \uadf8 \ud53d\uc140 \uac12\ub4e4\uc740 \ubaa8\ub450 \uc5ed\uce58\uac12\uc73c\ub85c \ubcc0\ud658. \uc791\uc73c\uba74 \uadf8\ub300\ub85c \ub460.\nAt this point it's not clear if truncate thresholding is an improvement compared to binary thresholding. Some cells are fused to each other while they should not be.\n\nOn the other hand. Adaptive thresholding methods apply a different threshold on different parts of the image, let's see how well it does on our images. See [here](https://docs.opencv.org/3.4/d7/d1b/group__imgproc__misc.html#gaa42a3e6ef26247da787bf34030ed772c) for more explanations.\nglobal Threshold\ub294 \ubb38\ud131 \uac12(=\uc5ed\uce58\uac12)\uc744 \ud558\ub098\uc758 \uc774\ubbf8\uc9c0 \uc804\uccb4\uc5d0 \uc801\uc6a9\uc2dc\ud0a4\ub294 \ubc18\uba74 adaptive Threshold\ub294 \uc774\ubbf8\uc9c0\uc758 \uad6c\uc5ed\uad6c\uc5ed\ub9c8\ub2e4 threshold\ub97c \uc2e4\ud589 \uc2dc\ucf1c\uc8fc\ub294 \uac83\uc774\ub2e4. \uadf8\ub7fc \uc544\ub798\uc758 \ud568\uc218\ub97c \ubcf4\uc790 \n\n\n\ncv2.adaptiveThreshold(img, value, adaptivemethod, thresholdType, blocksize, C)\n\n\n\nimg: grayscale \uc774\ubbf8\uc9c0\n\nvalue: adaptivemethod\uc5d0 \uc758\ud574 \uacc4\uc0b0\ub41c \ubb38\ud131 \uac12\uacfc thresholdType\uc5d0 \uc758\ud574 \ud53d\uc140\uc5d0 \uc801\uc6a9\ub420 \ucd5c\ub300 \uac12 \n\nadaptive method: \uc0ac\uc6a9\ud560 \ubb38\ud131\uac12 \uacc4\uc0b0 \uc54c\uace0\ub9ac\uc998 \n\n\n\ncv2.ADAPTIVE_THRESH_GAUSSIAN_C: X, Y\ub97c \uc911\uc2ec\uc73c\ub85c block Size * block Size \uc548\uc5d0 \uc788\ub294 \ud53d\uc140 \uac12\uc758 \ud3c9\uade0\uc5d0\uc11c C\ub97c \ubeb8 \uac12\uc744 \ubb38\ud131\uac12\uc73c\ub85c \ud568 \n\n\n\ncv2.ADAPTIVE_THRESH_MEAN_C: X, Y\ub97c \uc911\uc2ec\uc73c\ub85c block Size * block Size \uc548\uc5d0 \uc788\ub294 Gaussian \uc708\ub3c4\uc6b0 \uae30\ubc18 \uac00\uc911\uce58\ub4e4\uc758 \ud569\uc5d0\uc11c C\ub97c \ube80 \uac12\uc744 \ubb38\ud131\uac12\uc73c\ub85c \ud55c\ub2e4. \n\n\n\nblocksize: block * block size\uc5d0 \uac01\uac01 \ub2e4\ub978 \ubb38\ud131\uac12\uc774 \uc801\uc6a9\ub41c\ub2e4. \n\n\n\nC: \ubcf4\uc815 \uc0c1\uc218\ub85c\uc11c adaptive\uc5d0 \uacc4\uc0b0\ub41c \uac12\uc5d0\uc11c \uc591\uc218\uba74 \ube7c\uc8fc\uace0 \uc74c\uc218\uba74 \ub354\ud574\uc900\ub2e4. \n\n\n\n\ucd9c\ucc98: https://hoony-gunputer.tistory.com/99 [\ud6c4\ub2c8\uc758 \ucef4\ud4e8\ud130]\n", "source": "Kaggle", "docid": "Kaggle379", "language": "python", "num_cells": 23, "num_code_cells": 10, "num_md_cells": 13, "len_md_text": 4430, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "this kernel is just the beginning of a work in progress and will be updated very often. we will explore the dataset available for the human protein atlas image competition. questions we would like to answer include:* what channels of the"}, {"source_id": "ishandutta/sartorius-complete-unet-understanding", "name": "\ud83d\udd2cSartorius\ud83d\udd2c: Complete UNet Understanding", "file_name": "ishandutta_sartorius-complete-unet-understanding.ipynb", "html_url": "https://www.kaggle.com/code/ishandutta/sartorius-complete-unet-understanding", "description": "<h1><center>Sartorius: Complete UNet Understanding</center></h1>\n                                                      \n<center><img src = \"https://www.innovationnewsnetwork.com/wp-content/uploads/2021/02/%C2%A9-iStock-peterschreiber.media_-696x392.jpg\" width = \"750\" height = \"500\"/></center>                                                                            \n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents</center></h2>\n1. [Competition Overview](#competition-overview)    \n2. [Libraries](#libraries)  \n3. [Weights and Biases](#weights-and-biases)  \n4. [Global Config](#global-config)  \n5. [Load Datasets](#load-datasets)\n6. [U-Net Model](#unet-model)\n7. [References](#references)  \n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.</center></h3>\n<a id=\"competition-overview\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview</center></h2>\n## **<span style=\"color:orange;\">Description</span>**\n\n\nIn this competition, you\u2019ll detect and delineate distinct objects of interest in biological images depicting neuronal cell types commonly used in the study of neurological disorders. More specifically, you'll use phase contrast microscopy images to train and test your model for instance segmentation of neuronal cells. Successful models will do this with a high level of accuracy.\n\nIf successful, you'll help further research in neurobiology thanks to the collection of robust quantitative data. Researchers may be able to use this to more easily measure the effects of disease and treatment conditions on neuronal cells. As a result, new drugs could be discovered to treat the millions of people with these leading causes of death and disability.\n\n---\n## **<span style=\"color:orange;\">Evaluation Metric</span>**\n\nThis competition is evaluated on the **mean average precision** at different intersection over union  thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:\n\n`IoU(A,B)= (A\u2229B)/(A\u222aB)`\n\n---\n> The competition uses a couple of medical terms which many people will be unfamiliar with. To easen out the process I am providing the foundations of the mentioned diseases and proposed treatments to give a stronger domain understanding.\n<a id=\"libraries\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries</center></h2>\n<a id=\"weights-and-biases\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases (W&B)</center></h2>\n<center><img src = \"https://i.imgur.com/1sm6x8P.png\" width = \"750\" height = \"500\"/></center>  \n\n  \n \n  \n**Weights & Biases** is the machine learning platform for developers to build better models faster. \n\nYou can use W&B's lightweight, interoperable tools to \n- quickly track experiments, \n- version and iterate on datasets, \n- evaluate model performance, \n- reproduce models, \n- visualize results and spot regressions, \n- and share findings with colleagues. \n\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly. \n<a id=\"global-config\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config</center></h2>\n<a id=\"load-datasets\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets</center></h2>\n## **<span style=\"color:orange;\">Understanding the Structure of the Dataset</span>**\n\n> ### **<span style=\"color:orange;\">Goal of Competition</span>**\n> \n> In this competition we are segmenting neuronal cells in images. The training annotations are provided as run length encoded masks, and the images are in PNG format. The number of images is small, but the number of annotated objects is quite high. The hidden test set is roughly 240 images.\n> \n> ### **<span style=\"color:orange;\">Files</span>**\n> \n> **train.csv** - IDs and masks for all training objects. None of this metadata is provided for the test set.\n> \n> - `id` - unique identifier for object\n> \n> - `annotation` - run length encoded pixels for the identified neuronal cell\n> \n> - `width` - source image width\n> \n> - `height` - source image height\n> \n> - `cell_type` - the cell line\n> \n> - `plate_time` - time plate was created\n> \n> - `sample_date` - date sample was created\n> \n> - `sample_id` - sample identifier\n> \n> - `elapsed_timedelta` - time since first image taken of sample\n> \n> **sample_submission.csv** - a sample submission file in the correct format\n> \n> **train** - train images in PNG format\n> \n> **test** - test images in PNG format. Only a few test set images are available for download; the remainder can only be accessed by your notebooks when you submit.\n> \n> **train_semi_supervised** - unlabeled images offered in case you want to use additional data for a semi-supervised approach.\n> \n> **LIVECell_dataset_2021** - A mirror of the data from the LIVECell dataset. LIVECell is the predecessor dataset to this competition. You will find extra data for the SH-SHY5Y cell line, plus several other cell lines not covered in the competition dataset that may be of interest for transfer learning.\n>\n>---\n<a id=\"unet-model\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>U-Net Model</center></h2>\nThere is large consent that successful training of **deep networks** requires many thousand annotated training samples. In the UNet paper, researchers presented a network and training strategy that relies on the strong use of **data augmentation** to use the available annotated samples more efficiently. \n  \nThe architecture consists of a **contracting path** to capture context and a **symmetric expanding path** that enables precise localization. They showed that such a network can be trained end-to-end from very few images and outperforms the prior best method (*a sliding-window convolutional network*) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. \n  \nMoreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. \n  \n> The full\n> implementation (based on Caffe) and the trained networks are available\n> at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net\n---\n## **<span style=\"color:orange;\">Introduction to UNet</span>**\n\nIn the **UNet architecture** the upsampling part has a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the **expansive path** is more or less symmetric to the **contracting path**, and yields a u-shaped architecture. \n  \n>The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image.\n\n![](https://www.researchgate.net/publication/350484428/figure/fig2/AS:1006897591238656@1617074501235/U-Net-Overlap-tile-strategy-for-seamless-segmentation-of-arbitrary-large-images-here.ppm)\n\nThis strategy allows the seamless segmentation of arbitrarily large images by an\n**overlap-tile strategy** (above image). \n  \nTo predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. \n  \n>This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.\n  \nAs for the tasks there is very little training data available, researchers used excessive\n**data augmentation** by applying **elastic deformations** to the available training images. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. \n  \nThis is particularly important in *biomedical segmentation*, since deformation used to\nbe the most common variation in tissue and realistic deformations can be simulated efficiently. \n\n![](https://yann-leguilly.gitlab.io/img/unet_1/figure_3.png)\n\nAnother challenge in many cell segmentation tasks is the separation of touching objects of the same class (above image). To this end, researchers proposed the use of a **weighted loss**, where the separating background labels between touching cells obtain a large weight in the loss function.\n  \nThe resulting network is applicable to various biomedical segmentation problems.\n## **<span style=\"color:orange;\">UNet Architecture</span>**\n![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png)\nThe network architecture is illustrated above. \n  \nIt consists of a **Contracting Path** (left side) and an **Expansive Path** (right side). \n  \n1.**Contracting Path:**\n  \n>The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (*unpadded convolutions*), each followed by\na rectified linear unit (ReLU) and a 2x2 max pooling operation with *stride 2\nfor downsampling*. \n  \n>At each downsampling step we double the number of feature channels. \n\n2.**Expansive Path:**\n\n>Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (*up-convolution*) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. \n  \nThe cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. \n  \n*In total the network has 23 convolutional layers.*\n  \nTo allow a seamless tiling of the output segmentation map, it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size.\n---\n## **<span style=\"color:orange;\">Data Augmentation</span>**\n\n- Data augmentation is essential to teach the network the desired **invariance** and **robustness** properties, when only few training samples are available. \n  \n- In case of microscopical images we primarily need shift and rotation invariance as well as\nrobustness to deformations and gray value variations. \n  \n- Especially **random elastic deformations** of the training samples seem to be the key concept to train a segmentation network with very few annotated images. \n  \n- We generate smooth deformations using random **displacement vectors** on a coarse 3 by 3 grid. \n  \n- The displacements are sampled from a **Gaussian distribution** with 10 pixels standard deviation. \n  \n- Per-pixel displacements are then computed using **bicubic interpolation**. \n  \n- Drop-out layers at the end of the contracting path perform further implicit data augmentation.\n\n---\n## **<span style=\"color:orange;\">Utilities</span>**\n## **<span style=\"color:orange;\">Dataset Class</span>**\n## **<span style=\"color:orange;\">Dataset Loader</span>**\n## **<span style=\"color:orange;\">Losses</span>**\n### **<span style=\"color:orange;\">1. Focal Loss</span>**\n\nFocal loss was introduced by Lin et al. in [Focal Loss for Dense Object Detection](https://paperswithcode.com/paper/focal-loss-for-dense-object-detection)\n\n- A **Focal Loss** function addresses class imbalance during training in tasks like object detection. \n  \n- Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples. \n  \n- It is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. \n  \n- Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples.\n\n![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_4.45.06_PM_leJm2yh.png)\n### **<span style=\"color:orange;\">2. Focal Loss</span>**\n## **<span style=\"color:orange;\">Model</span>**\n## **<span style=\"color:orange;\">Training</span>**\n## [Check out the run page here $\\rightarrow$](https://wandb.ai/ishandutta/sartorius-unet?workspace=)  \n<a id=\"references\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References</center></h2>\n>- [\ud83e\udda0 Sartorius - Starter Baseline Torch U-net](https://www.kaggle.com/julian3833/sartorius-starter-baseline-torch-u-net)\n>- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://paperswithcode.com/paper/u-net-convolutional-networks-for-biomedical)\n>\n>---\n<h1><center>More Plots and Models coming soon!</center></h1>\n\n<center><img src = \"https://static.wixstatic.com/media/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg/v1/fill/w_934,h_379,al_c,q_90/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg\" width = \"750\" height = \"500\"/></center> \n--- \n\n## **<span style=\"color:orange;\">Let's have a Talk!</span>**\n> ### Reach out to me on [LinkedIn](https://www.linkedin.com/in/ishandutta0098)\n\n---\n", "source": "Kaggle", "docid": "Kaggle250", "language": "python", "num_cells": 59, "num_code_cells": 21, "num_md_cells": 38, "len_md_text": 14601, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "h1>center>Sartorius: Complete UNet Understanding/h3>. width = 750\" height = \"500\"/> > '500' width= \"550\" width: \""}, {"source_id": "banaabdallah/hpa-pred-an-3", "name": "HPA_pred_AN_3", "file_name": "banaabdallah_hpa-pred-an-3.ipynb", "html_url": "https://www.kaggle.com/code/banaabdallah/hpa-pred-an-3", "description": "The idea of Below cell block (faster_HPA_cell_segmentor) goes to: https://www.kaggle.com/linshokaku/faster-hpa-cell-segmentation/output\n\nBut the code has been modified a little to adapt my approach.\n", "source": "Kaggle", "docid": "Kaggle589", "language": "python", "num_cells": 14, "num_code_cells": 13, "num_md_cells": 1, "len_md_text": 198, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "Below cell block (faster_HPA_cell_segmentor) has been modified a little to adapt my approach."}, {"source_id": "nsff591/popular-ml-nn-cnn-rnn-model-code-snippets", "name": "Popular ML/NN/CNN/RNN Model code snippets \ud83e\udd16\ud83d\udcc8", "file_name": "nsff591_popular-ml-nn-cnn-rnn-model-code-snippets.ipynb", "html_url": "https://www.kaggle.com/code/nsff591/popular-ml-nn-cnn-rnn-model-code-snippets", "description": "# Popular ML/NN/CNN/RNN Models code snippets  \ud83e\udd16\ud83d\udcc8\n<a id=\"cell-intro\"></a>\n# 1. Introduction\nThis cheatsheet contains:\n\n1.   **Popular Machine Learning models**\n2.   **(Deep) Neural Networks**\n3.   **Convolutional Neural Networks**\n4.   **Recurrent Neural Networks**\n5.   **Natural Language Processing**\n6.   **Data Engineering and Preprocessing**\n7.   **Statistical tools**\n\nFeel free to copy paste my code snippets in your project to jump start it. This cheatsheet will not go into detail of how every model/architecture works, nor will it cover all the parameters of each model. But it will give you a great start for most common problems. Appropriate data for each model has been provided if you want to change something for yourself.\n\nI occasionaly wrote Tips/explanations for some models in case you forgot it's characteristics. Always import \"[\u26a0\ufe0f Global Libraries for every project \u26a0\ufe0f](#cell-global_lib)\" first to make anything else work within this notebook.\n\nIf this project was helpful, feel free to upvote and/or reference my work \ud83d\udc4d. If you want me to add more or change something, please comment and I will look into it.\n<a id=\"cell-toc\"></a>\n# 2. Table of Content\n1 [Introduction](#cell-intro)\n\n2 [Table of Content](#cell-toc)\n\n3 [Model Explanation and Implementation](#cell-models)\n\n[\u26a0\ufe0f Global Libraries for every project \u26a0\ufe0f](#cell-global_lib)\n\n3.1 [Basic Regression](#cell-basic_regression)\n\n* [Importing simple regression data](#cell-basic_regression_import)\n1.  [Linear Regression](#cell-basic_linear_regression)\n2.  [Multi-linear regression](#cell-basic_multi_linear_regression)\n3.  [Polynomial regression](#cell-basic_poly_regression)\n4.  [Decision tree regression](#cell-basic_dtr)\n5.  [Random forest regression](#cell-basic_rfr)\n6.  [Support Vector regression](#cell-basic_svmr)\n\n3.2 [Basic Classification](#cell-basic_c)\n\n* [Importing Simple classification data](#cell-basic_c_data)\n\n1.  [Logistic regression](#cell-basic_c_lr)\n2.  [Naive bayes](#cell-basic_c_nb)\n3.  [K nearest neighbours(KNN)](#cell-basic_c_knn)\n4.  [Suport vector machine (SVM)](#cell-basic_c_svm)\n5.  [Kernel SVM](#cell-basic_c_ksvm)\n6.  [Decision tree classification](#cell-basic_c_dtc)\n7.  [Random forest classification](#cell-basic_c_rfc)\n____\n*(the following models can also be used for regression)*\n\n8.  [LightLGBM model](#cell-basic_c_lgbm)\n9.  [XGBoost model](#cell-basic_c_xgbm)\n10. [Catboost model](#cell-basic_c_cbm)\n\n3.3 [Neural Networks (NN)](#cell-nn)\n\n* [Importing Simple NN data](#cell-nn_data)\n\n1. [Neural Network: Sequential() keras](#cell-nn_seq)\n2. [Neural Network: Functional Keras](#cell-nn_func)\n3. [Neural Network: PyTorch](#cell-nn_pytorch)\n4. [Self Organizing Maps (SOM)](#cell-nn_som)\n5. [Boltzman Machine](#cell-nn_bm)\n6. [AutoEncoders](#cell-nn_ae)\n\n3.4 [Convolutional Neural Networks (CNN)](#cell-cnn)\n\n* [Importing Simple CNN data](#cell-cnn_data)\n\n1. [Convolutional Neural Network: Sequential() keras](#cell-cnn_seq)\n2. [Convolutional Neural Network: Functional keras](#cell-cnn_func)\n3. [Convolutional Neural Network: PyTorch](#cell-cnn_pytorch)\n4. [Skip Connections/Residual blocks](#cell-cnn_skip)\n5. [Inception Network](#cell-cnn_incept)\n6. [Depthwise separable convolution](#cell-cnn_dsc)\n7. [EfficientNet (Scaling models based on computational needs)](#cell-cnn_eff)\n8. [Transfer Learning](#cell-cnn_tran)\n9. [Object Detection](#cell-cnn_obj)\n10. [Semantic Segmentation](#cell-cnn_semantic_seg)\n11. [Face Recognition](#cell-cnn_face_rec)\n12. [Neural Style Transfer](#cell-cnn_style_transf)\n\n* [Importing Simple CNN data](#cell-cnn_data)\n\n3.5 [Recurrent Neural Networks (RNN)](#cell-rnn)\n\n* [Importing Simple RNN data](#cell-rnn_data)\n1. [Long short-term memory (LSTM)](#cell-rnn_lstm)\n\n3.6 [Natural Language Processing (NLP)](#cell-nlp)\n1. [Sentiment Analysis](#cell-nlp_sent)\n2. [Question/Answering](#cell-nlp_qa)\n3. [Information retrieval](#cell-nlp_ir)\n4. [Information extraction](#cell-nlp_ie)\n5. [Machine Translation](#cell-nlp_mt)\n\n4 [Data engineering and Preprocessing Tricks](#cell-data_engineer_and_preprocessing)\n\n* [Importing Simple Data Engineering data](#cell-data_eng_data)\n\n5 [Statistical Tools](#cell-stat)\n\n6 [References](#cell-references)\n<a id=\"cell-models\"></a>\n# 3. Model Explanation and Implementation\n<a id=\"cell-global_lib\"></a>\n## \u26a0\ufe0f Global Libraries for every project \u26a0\ufe0f\n<a id=\"cell-basic_regression\"></a>\n## 3.1 Basic Regression\n<a id=\"cell-basic_regression_import\"></a>\n### Importing Simple Regression data\n<a id=\"cell-basic_linear_regression\"></a>\n### 3.1.1. Linear Regression\n<img src=\"https://www.reneshbedre.com/assets/posts/reg/reg_front.svg\" width=\"650\" align=\"centr\"/>\n<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/8119b3ed1259aa8ff15166488548104b50a0f92e\" width=\"150\" align=\"centr\"/>\nLinear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable. The model will alter the \u03b2 and \u03b5 to fit the model.\nNote: R\u00b2 cannot be used for nonlinear regression but is a good indicator for linear regressions. (the closer to 1 the better)\nIf we would have a perfect straight line, this would mean R\u00b2 would be 1 and our predictive model would have been 100% accurate. This is not the case but the shape of the scatter plot does show that we can predict pretty close to it and we even notice some outliers which can be tracked down with further research on the dataset if we wanted to.\n<a id=\"cell-basic_multi_linear_regression\"></a>\n### 3.1.2. Multi-linear regression\n**Difference between Linear and multi-linear regression:**\n\nSimple linear regression has only one x and one y variable.\nMultiple linear regression has one y and two or more x variables/features.\n\nWe actually use a multi-linear dataset here, which means that our previous **Linear regression code is the same for the multi-linear version.** \n<a name=\"cell-basic_poly_regression\"></a>\n### 3.1.3. Polynomial regression\n<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/machine-learning-polynomial-regression.png\" width=\"650\" align=\"centr\"/>\n<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/bc6e10cc75097fa66e7e02d6a75491d14a0c4aba\" width=\"500\" align=\"centr\"/>\n<a id=\"cell-basic_dtr\"></a>\n### 3.1.4. Decision tree regression\n<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_001.png\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-basic_rfr\"></a>\n### 3.1.5. Random forest regression\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-basic_svmr\"></a>\n### 3.1.6. Support Vector regression\n<img src=\"https://i.stack.imgur.com/29nu8.png\" width=\"650\" align=\"centr\"/>\n**Feature scaling is needed with SVM**\n<a id=\"cell-basic_c\"></a>\n## 3.2 Basic Classification\n<a id=\"cell-basic_c_data\"></a>\n### Importing Simple classification data\n<a id=\"cell-basic_c_lr\"></a>\n### 3.2.1. Logistic regression\n<img src=\"https://rajputhimanshu.files.wordpress.com/2018/03/linear_vs_logistic_regression.jpg\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-basic_c_nb\"></a>\n### 3.2.2. Naive Bayes\n<img src=\"https://thatware.co/wp-content/uploads/2020/04/naive-bayes.png\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-basic_c_knn\"></a>\n### 3.2.3. K-nearest neighbours (KNN)\n<img src=\"https://raw.githubusercontent.com/artifabrian/dynamic-knn-gpu/master/knn.png\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-basic_c_svm\"></a>\n### 3.2.4. Suport vector machine (SVM)\n<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-basic_c_ksvm\"></a>\n### 3.2.5. Kernel SVM\n<img src=\"https://slidetodoc.com/presentation_image_h/2f087c491fa6549a8cb25688253dec9b/image-14.jpg\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-basic_c_dtc\"></a>\n### 3.2.6. Desicion tree classification\n<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/decision-tree-classification-algorithm2.png\" width=\"650\" align=\"centr\"/>\nWeak against overfitting, use random forest to prevent that, if that is the case\n<a id=\"cell-basic_c_rfc\"></a>\n### 3.2.7. Random forest classification\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-basic_c_lgbm\"></a>\n### 3.2.8. LightLGBM model\n<img src=\"https://miro.medium.com/max/3000/1*AZsSoXb8lc5N6mnhqX5JCg.png\" width=\"650\" align=\"centr\"/>\n<img src=\"https://miro.medium.com/max/3000/1*whSa8rY4sgFQj1rEcWr8Ag.png\" width=\"650\" align=\"centr\"/>\nLight GBM is a gradient boosting framework that uses tree based learning algorithm.\n\nLight GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n\nWhy is it good? \n\n\n\n*   High speed\n*   Can handle large datasets\n*   low memory use\n*   supports gpu learning\n\n\nSome extra explanation on the parameters if needed:\nhttps://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n<a id=\"cell-basic_c_xgbm\"></a>\n### 3.2.9. XGBoost\n<img src=\"https://www.researchgate.net/profile/Li-Mingtao-2/publication/335483097/figure/fig3/AS:934217085100032@1599746118459/A-general-architecture-of-XGBoost.ppm\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-basic_c_cbm\"></a>\n### 3.2.10. CatBoost\n<img src=\"https://miro.medium.com/max/875/1*E006sjlIjabDJ3jNixRSnA.png\" width=\"650\" align=\"centr\"/>\n\nsource: https://medium.com/riskified-technology/xgboost-lightgbm-or-catboost-which-boosting-algorithm-should-i-use-e7fda7bb36bc\n\n<img src=\"https://i1.wp.com/thaddeus-segura.com/wp-content/uploads/2020/10/cb9.png?resize=923%2C262&ssl=1\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-nn\"></a>\n## 3.3 Neural Networks (NN)\n<a id=\"cell-nn_data\"></a>\n### Importing Simple Data\nThis dataset is a classification problem, so we will have to Encode the dataset using the OneHotEncoder.\n<a id=\"cell-nn_seq\"></a>\n### 3.3.1 Neural Network: Sequential() keras\n<img src=\"https://nickmccullum.com/images/python-deep-learning/how-do-neural-networks-really-work/completed-neural-network.png\" width=\"650\" align=\"centr\"/>\nThis code snipet contains a simple sequential keras model. It can be used for simple models which only have one input and output. Otherwise you should use functional keras API.\n\nActivation functions commonly used:\n\n<img src=\"https://www.researchgate.net/profile/Aaron-Stebner-2/publication/341310767/figure/fig7/AS:890211844255749@1589254451431/Common-activation-functions-in-artificial-neural-networks-NNs-that-introduce.ppm\" width=\"650\" align=\"centr\"/>\n\nAnalysis on Training the model:\n\nIf your training (blue line) starts to heavily outperform the validation (yellow line), then you are overfitting. Solutions against this:\n\n* Change learning rate higher/lower OR change it to a decaying learning rate\n* Regularize your model by using BatchNormalization and or Dropout\n* Data Engineer your dataset, to become more robust (example: binning)\n* Make the neural network smaller\n\nDeciding how big your neural network should be:\n\n* Make your neural network bigger as long as it does not overfit the data on Val/Test-dataset\n* The more data you have, the bigger your model can become without overfitting\n* Don't make too big of a NN if you don't have a lot of data. Example: having 1k dataset but 1million parameters in the model will almost always create a bad model. But it is normal that you have more parameters than data samples, just don't go overboard and start thinking about regularization if it does get out of hand. Regularization helps a lot with bigger models.\nPost processing for this specific dataset. This might be different for a case by case basis.\nAnalysis of the result\n<a id=\"cell-nn_func\"></a>\n### 3.3.2 Neural Network: Functional Keras\n<img src=\"https://www.researchgate.net/publication/330230427/figure/fig5/AS:962670173892631@1606529863497/A-multi-layer-neural-network-with-n-inputs-at-least-two-hidden-layers-and-one-output.png\" width=\"650\" align=\"centr\"/>\nYou should use Functional keras if you have more complex models with multiple inputs and outputs. I will use an example which was widely used during the https://www.kaggle.com/competitions/ubiquant-market-prediction competition where 1 feature had it's own model and got merged with the other 300 features later in the model. The reason being, that the 1 feature had a significant meaning to the outcome of the output value. This did not always mean a better performance, but it was a great idea.\n\nI would also recommend using functional keras for complex Convolutional Neural Networks but more on that later.\nAltering previous dataset to fit our model.\n<a id=\"cell-nn_pytorch\"></a>\n### 3.3.3 Neural Network: PyTorch\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n<a id=\"cell-nn_som\"></a>\n### 3.3.4 Self Organizing Maps (SOM)\n\ud83d\udd27  Work in Progress \ud83d\udd27\n<a id=\"cell-nn_bm\"></a>\n### 3.3.5 Boltzman Machine\n\ud83d\udd27  Work in Progress \ud83d\udd27\n<a id=\"cell-nn_ae\"></a>\n### 3.3.6 AutoEncoders\n\ud83d\udd27  Work in Progress \ud83d\udd27\n<a id=\"cell-cnn\"></a>\n## 3.4 Convolutional Neural Networks (CNN)\n<a id=\"cell-cnn_data\"></a>\n### Importing Simple Data\n<a id=\"cell-cnn_seq\"></a>\n## 3.4.1 Convolutional Neural Networks: Sequential() keras\n<img src=\"https://miro.medium.com/max/1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg\" width=\"650\" align=\"centr\"/>\n**Convolution:** Is applying a filter on an image to capture for example \"edges\" or \"patterns\" of images. These filters can be set but are often parameters and trained by the model.\n\n<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_02_17A-ConvolutionalNeuralNetworks-WHITEBG.png\" width=\"650\" align=\"centr\"/>\n**Padding:** Padding is making the previous image/input bigger by adding n-\"empty blocks\" around the image/input. This allows our convolution or pooling to capture edges better. If people use \"same\"-padding. They choose a specific amount of n-\"empty blocks\" to add, which would generate the same dimension of output after convolving/pooling as they had received as input.\n\n<img src=\"https://miro.medium.com/max/666/1*noYcUAa_P8nRilg3Lt_nuA.png\" width=\"650\" align=\"centr\"/>\n**Stride:** The speed at which your filter/pooling goes through your image/input. If they say: Stride=2. This means that you would go 2 steps to the right every filter/pooling instead of 1 step.\n**Pooling:** There are 2 commen pooling methods:\n\n* Max pooling: Take the maximum of an element within that pool matrix\n* Average pooling: Average over all the elements within that pool matrix\n\n<img src=\"https://miro.medium.com/max/1192/1*KQIEqhxzICU7thjaQBfPBQ.png\" width=\"650\" align=\"centr\"/>\nThere is a trend to lower the dimensions of the image throughout the CNN and increase the amount of filters throughout the model. This is based on LeNet-5's research paper but I am not qualified to say if this is usefull or good in all CNN problems.\n\n<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-52-17.png\" width=\"650\" align=\"centr\"/>\nEvaluating the model\n<a id=\"cell-cnn_func\"></a>\n## 3.4.2 Convolutional Neural Networks: Functional keras\n<a id=\"cell-cnn_pytorch\"></a>\n## 3.4.3 Convolutional Neural Networks: PyTorch\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n<a id=\"cell-cnn_skip\"></a>\n## 3.4.4 Skip Connections/Residual blocks\n<img src=\"https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png\" width=\"650\" align=\"centr\"/>\n\n<img src=\"https://theaisummer.com/static/2c373d3667071700748bf451c4e62b78/3accd/long-skip-connection.jpg\" width=\"650\" align=\"centr\"/>\n\nSkip connections/Residual blocks are used to solve the performance degradation problem associated with deep neural architectures. (This solution was introduced with the ResNet paper) See graph below to show the problem that it will prevent.\n\n<img src=\"https://miro.medium.com/max/1280/1*Ku0ChYxemQyF1hz348ExVA.png\" width=\"650\" align=\"centr\"/>\nIt can sometimes be usefull to make an **Identity Block** method mentioned in the ResNet paper to easily make a 2-layer/3-layer skip. In an identy block, your input dimensions should be te same as the output. However, if this is not the case you can add a conv layer in the skip to compensate this dimension difference. This is called a **Convolution Block**\n\n<img src=\"https://i.stack.imgur.com/37qzA.png\" width=\"650\" align=\"centr\"/>\nWe have too little data to justify using this big of a model, hence the worse accuracy result (also the model is pretty bad to be honest). Skip connections/residual blocks should only be used in very deep neural networks. I hope the example code could at least help jumpstart your specific problem.\n<a id=\"cell-cnn_incept\"></a>\n### 3.4.5 Inception Network/Network in Network\nUsing a 1x1 filter + Activation is usefull to shrink/enlarge the input. This could result in lower computation power. This is used in the inception network to lower the computation power needed before concatenating the different matricis together. The use of a 1x1 Conv is sometimes also referred to a \"**Bottleneck**\".\n\n<img src=\"https://i.ytimg.com/vi/KfV8CJh7hE0/maxresdefault.jpg\" width=\"650\" align=\"centr\"/>\n\n<img src=\"https://miro.medium.com/max/1276/1*qVQbA9GYe5VKIQtAQWPM9w.jpeg\" width=\"650\" align=\"centr\"/>\nThe auxiliary classifiers in the GooLeNet help regularize the network and provide extra classifiers which might be usefull in ensemble models.\n<a id=\"cell-cnn_dsc\"></a>\n### 3.4.6 Depthwise separable convolution\nBy using the depthwise and pointwise convolution we create the depthwise seprable conv used in MobileNet v1 for low computing applications. In MobileNet v2 a skip connection over the block was added and a pointwise conv before the depthwise conv within the block. (also known as **bottleneck block**) (see picture)\n\n<img src=\"https://miro.medium.com/max/448/1*fE-1I6D8A4B9QAUgZEbLUA.png\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-cnn_eff\"></a>\n### 3.4.7 EfficientNet (Scaling models based on computational needs)\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n<a id=\"cell-cnn_tran\"></a>\n### 3.4.8 Transfer Learning\nThe idea of transfer learning is taking another model and deleting the output layer or several of it's layers/weights to train them again on your dataset and your specific output. This works well in CNN's because grasping features like edges and higher dimensional filters, are generally the same for most image based problems. This causes us to benefit from others well trained models.\n\n<img src=\"https://miro.medium.com/max/1400/1*f2_PnaPgA9iC5bpQaTroRw.png\" width=\"650\" align=\"centr\"/>\nWe are going to use the MobileNetV2 model to transfer learn from to classify alpaca's.\nLast 2 layers of the model is considered as the top layer, if you had included the Top in previous code as True you can read them with below code. (uncomment if you want to see it)\nFreezing (part of) the model is to prevent it to relearn everything again.\n<a id=\"cell-cnn_obj\"></a>\n## 3.4.9 Object Detection\nTo understand Object Detection, we first need to refresh a few concepts.\n\nWe first want to detect the object and then classify it. This can be done all within one model, by representing our output layer as the following:\n\n**[Pc,bx,by,bh,bw,C1,C2,C3,...]**\n\nMeanings:\n\n* **Pc**(is there an object or not?)\n* **bx**(bounding box X value)\n* **by**(bounding box Y value)\n* **bh**(bounding box height)\n* **bw**(bounding box Width)\n* **C1**(class 1),**C2**(class 2),**C3**,...\n\nIf we want to detect multiple objects at the same time, we can add another of the above group to the output layer untill we are satisfied. For example: to detect 5 objects at the same time, we need 5 [Pc,bx,by,bh,bw,C1,C2,C3,...] concatenated together as output layer.\n\nIf we want to detect Landmarks. We remove the bh and bw out of the output and just save the x and y locations of each landmark instead. (example: face recognition)\n\nTo detect the position of the object, we don't want to check every pixel for an object. This would be computationally inefficient. The solution to this is splitting the image into smaller parts and only checking those for an object. This has several implementation and solutions.\n\n**Sliding windows**\n\n<img src=\"https://i.stack.imgur.com/tQZI2.jpg\" width=\"650\" align=\"centr\"/>\n\n**YOLO**\n\n(Yolo also makes use of anchor boxes, more on that later)\n\n<img src=\"https://miro.medium.com/max/1152/1*m8p5lhWdFDdapEFa2zUtIA.jpeg\" width=\"650\" align=\"centr\"/>\n\n**Non-max Suppression**\nNon-max Suppression looks at all the possible bounding boxes around an object (Intersection over Union smaller than 0.5 for example) and takes the one with the highest probability.\n\n<img src=\"https://2628535719-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-M5-0RGo4dZhdwCIHrC1%2F-M5-0TEuN77zAmTpQjIr%2F-M5-0WMNj3vkyPeFge_4%2FNon-Max%20Suppression.JPG?generation=1586990829149087&alt=media\" width=\"650\" align=\"centr\"/>\n\n**Intersection over Union**\n\n<img src=\"https://media5.datahacker.rs/2018/11/IoU.png\" width=\"650\" align=\"centr\"/>\n\n**Anchor Boxes**\n(used in YOLO)\nBy adding different styles of boxes, we can detect multiple objects of different sizes within the \"image fragment\" we are looking at.\n\n<img src=\"https://media5.datahacker.rs/2018/11/ancor_1-1.png\" width=\"650\" align=\"centr\"/>\n\nI first thought about using YOLO from https://github.com/miranthajayatilake/YOLOw-Keras.git , but you should probably use better performing detection models like Yolo v5: https://github.com/ultralytics/yolov5 for better performance. (Better models will probably pop up in the future, so this is still an ever changing field)\n\nThe Idea of this project is:\n* Loading Yolo model\n* Deleting Top layer and replacing it with our own output layers\n* Maybe retraining some of the later layers by setting \"trainable = True\"\n* Making sure input images are preprocessed into the right size (width, height)\n\n\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n\n<a id=\"cell-cnn_semantic_seg\"></a>\n## 3.4.10 Semantic Segmentation\nSemantic segmentation is known for predicting classes on images, pixel-by-pixel. (like cars or the road)\n\n<img src=\"https://miro.medium.com/max/800/1*WKwbz04uLR0ds5M0xiCdjg.jpeg\" width=\"650\" align=\"centr\"/>\n\n**Transposed Convolution**:\n\n<img src=\"https://preview.redd.it/swxoctxkj9341.jpg?width=1210&format=pjpg&auto=webp&s=3c1e3301fae700f730b8ace194f33331ada2bdad\" width=\"650\" align=\"centr\"/>\n\nThe transposed convolution is used to upscale the dimensions in the **U-Net** architecture.\n\n<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"650\" align=\"centr\"/>\nThe following code is heavily inspired by coursera deep learning specialization and https://www.kaggle.com/code/oluwatobiojekanmi/carla-image-semantic-segmentation-with-u-net . \n\n***Disclaimer: For those who think this is a solution for the coursera course, I altered the code so it has an educational purpose instead of a copy paste solution. This is not a solution to the course, so don't even try.***\nCreating the model by making encoding blocks (downsampling) and decoding blocks (upsampling)\n<a id=\"cell-cnn_face_rec\"></a>\n## 3.4.11 Face Recognition\nTo recognize someone, we make use of the triplet loss function **Triplet loss** is a loss function for machine learning algorithms where a reference input (called anchor) is compared to a matching input (called positive) and a non-matching input (called negative).\n\nThe \"distance\" between these are being compared and if they are below/above a certain treshhold, they will result as a match or not.\n\n<img src=\"https://miro.medium.com/max/1302/1*SKWGC3ehCbGCsbJVge6kmg.png\" width=\"650\" align=\"centr\"/>\n\nI recommend using **FaceNet** model and transfer learn it for your project to start out.\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n<a id=\"cell-cnn_style_transf\"></a>\n## 3.4.12 Neural Style Transfer\nWe take an image (content) and merge it together with a specific style (another image).\n\n<img src=\"https://hackernoon.com/hn-images/1*k5Q_NYr1niC-qjWMr-lUCg.png\" width=\"650\" align=\"centr\"/>\n\nWe calculate the cost function based on following formula between the style and the content:\n\n<img src=\"https://assets.website-files.com/5ac6b7f2924c652fd013a891/5ddd8faa5ec068074fc684b0_Tcyrhi9Soqdop2OAXQoz2yg25nCsdrBNAWWTYzdNK83V8srvlMJJv9KXmQR9PC6Pa_ktiwdvdc-CBhRNX_CsaQcl0oKS92_gSjDj0q9xKigaipvuqQHWFAtEE6a3ulK_znVZ_tI.png\" width=\"650\" align=\"centr\"/>\n\n<img src=\"https://miro.medium.com/max/1294/1*ZgW520SZr1QkGoFd3xqYMw.jpeg\" width=\"650\" align=\"centr\"/>\n\n**Approach**: We load a pre-trained model like VGG/ImageNet/MobileNet/ResNet remove the top and implement the cost function around the middle layers of the pretrained model. (This works as these pretrained models have well trained layers to capture features of the image) We try to capture both the shallow and deeper features by implementing it around the middle layers of the model.\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n<a id=\"cell-rnn\"></a>\n## 3.5 Recurrent Neural Networks (RNN)\n<a id=\"cell-rnn_data\"></a>\n### Importing Simple Data\n<a id=\"cell-rnn_lstm\"></a>\n## 3.5.1 Long short-term memory (LSTM)\n<img src=\"https://miro.medium.com/max/674/1*jikKbzFXCq-IYnFZankIMg.png\" width=\"650\" align=\"centr\"/>\n<a id=\"cell-nlp\"></a>\n## 3.6 Natural Language Processing (NLP)\n<a id=\"cell-nlp_sent\"></a>\n## 3.6.1 Sentiment Analysis\nProject approach:\n\n* Clean the data by deleting Stopwords, Stemming and put it all into the \"corpus\"\n* Tokenize your corpus with \"bag of words\" or term frequency-inverse document frequency (TF-IDF)\n* Train the model\n\n**Corpus**: A corpus is a collection of authentic text or audio organized into dataset.\n\n**Stemming**: Stemming is the process of reducing a word to one or more stems. A stemming dictionary maps a word to its lemma (stem)\n\n**Tokenization**: Tokenization is the process of representing raw text in smaller units called tokens. These tokens can then be mapped with numbers to further feed to an NLP model\n\nTF-IDF: \n\n<img src=\"https://miro.medium.com/max/661/1*3K9GIOVLNu0cRvQap_KaRg.png\" width=\"650\" align=\"centr\"/>\n\nAs I did not cover the BERT model, which can be used for a wide variety of NLP tasks, I would like to recommend to also check this notebook as a better coverage of the topic. https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained\n<a id=\"cell-nlp_qa\"></a>\n## 3.6.2 Question/Answering\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n<a id=\"cell-nlp_ir\"></a>\n## 3.6.3 Information retrieval\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n<a id=\"cell-nlp_ie\"></a>\n## 3.6.4 Information extraction\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n<a id=\"cell-nlp_mt\"></a>\n## 3.6.5 Machine Translation\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n<a id=\"cell-data_engineering_and_preprocessing\"></a>\n# 4. Data engineering and Preprocessing Tricks\n<a id=\"cell-data_eng_data\"></a>\n### Importing Simple data engineering data\n\ud83d\udd27  Work in Progress \ud83d\udd27 \nLog transformation\nBinning\nK-means\ndata augmentation\nStratifiedKfold\noversampling: SMOTE and ADASYN\nUndersampling\nPrefetch Autotune\n1.   PCA\n2.   Kernel PCA\n3.   Linear discriminant Analysis (LDA)\n<a id=\"cell-stat\"></a>\n# 5. Statistical Tools\nEDA tools: https://python-graph-gallery.com/\n\ud83d\udd27  Work in Progress \ud83d\udd27 \n\nRMSE\n\nX\u00b2\n\ncross-entropy\n\nvariance\n\nstandard deviation\n\ncovariance matrix\n\ncorrelation\n\n<a id=\"cell-references\"></a>\n# 6. References\nI would like to give credit to all dataset providers and the udemy/coursera courses who taught me most of my knowledge. I would also like to thank the kaggle community to providing educational projects that helped me better understand the concepts within Artificial Intelligence.\n\nSome code was heavily inspired/taken from the following courses:\n* https://www.udemy.com/course/machinelearning/\n* https://www.udemy.com/course/deeplearning/\n* https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/\n* https://www.coursera.org/specializations/deep-learning\n\n***Disclaimer: For those who think this is a solution for the coursera course, I altered the code so it has an educational purpose instead of a copy paste solution. This is not a solution to the course, so don't even try.***\n\n", "source": "Kaggle", "docid": "Kaggle403", "language": "python", "num_cells": 258, "num_code_cells": 113, "num_md_cells": 145, "len_md_text": 27962, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "this cheatsheet contains:1. **Popular Machine Learning models**2. **(Deep) Neural Networks**3. **Recurrent Neal Network**5. **Data Engineering and Preprocessing**7. **Statistical tools**F"}, {"source_id": "ayushdabra/inceptionresnetv2-unet-81-dice-coeff-86-acc", "name": "InceptionResNetV2-UNet (81% Dice Coeff. & 86% Acc)", "file_name": "ayushdabra_inceptionresnetv2-unet-81-dice-coeff-86-acc.ipynb", "html_url": "https://www.kaggle.com/code/ayushdabra/inceptionresnetv2-unet-81-dice-coeff-86-acc", "description": "# Dubai Satellite Imagery Semantic Segmentation\nHumans in the Loop has published an open access dataset annotated for a joint project with the Mohammed Bin Rashid Space Center in Dubai, the UAE. \n\nThe dataset consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. The images were segmented by the trainees of the Roia Foundation in Syria.\n\nOriginal Dataset Link: https://humansintheloop.org/resources/datasets/semantic-segmentation-dataset/\n# Installing & Importing Libraries\n# Data Augmentation using Albumentations Library\n[Albumentations](https://albumentations.ai/) is a Python library for fast and flexible image augmentations. Albumentations efficiently implements a rich variety of image transform operations that are optimized for performance, and does so while providing a concise, yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection.\n\nData augmentation is done by the following techniques:\n\n1. Random Cropping\n2. Horizontal Flipping\n3. Vertical Flipping\n4. Rotation\n5. Random Brightness & Contrast\n6. Contrast Limited Adaptive Histogram Equalization (CLAHE)\n7. Grid Distortion\n8. Optical Distortion\n# Saving Augmented Images to Disk\nI have already performed data augmentation and saved the images. I am not running it's code in this notebook. It is a very time consuming process, so be patient while the code cell runs!\n# Working with Augmented Dataset\n# Create Useful Label & Code Conversion Dictionaries\n\nThese will be used for:\n\n* One hot encoding the mask labels for model training\n* Decoding the predicted labels for interpretation and visualization\n# Define Functions for One Hot Encoding RGB Labels & Decoding Encoded Predictions\n# Creating Custom Image Data Generators\n## Defining Data Generators\n# Custom Image Data Generators for Creating Batches of Frames and Masks\n# Model\n## InceptionResNetV2-UNet\n### Model Layout\n# Model Training\n## Training Results\n## Predictions\n## Model's Activations (Outputs) Visualization\n", "source": "Kaggle", "docid": "Kaggle337", "language": "python", "num_cells": 54, "num_code_cells": 38, "num_md_cells": 16, "len_md_text": 2085, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "the dataset consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. the images were segmented by the trainees of the Roia Foundation"}, {"source_id": "arashmehrzadi/brain-tumor-segmentation-unet", "name": "Brain_Tumor_Segmentation_UNET", "file_name": "arashmehrzadi_brain-tumor-segmentation-unet.ipynb", "html_url": "https://www.kaggle.com/code/arashmehrzadi/brain-tumor-segmentation-unet", "description": "# Brain Tumor Segmentation  Unet\n![](https://files.miamineurosciencecenter.com/media/filer_public_thumbnails/filer_public/76/6b/766b99f3-7395-4300-b031-1a04b4bc0e24/brain_tumor_diagnosis.jpeg__1280x800_q85_crop_subsampling-2.jpg)\n**The process of segmenting tumor from MRI image of a brain is one of the highly focused areas in the community of medical science as MRI is noninvasive imaging. Therefore, I decided to create Brain Tumor Segmentation Notebook  that you can easily use the capabilities of this powerful tool. I am currently in the early stages of designing this Notebook and will be constantly updating these codes to get to a usable version.**\n**If you are interested in working in this field or cooperating with me, you can contact me through the following links :**\n\n**Email :  mehrzadiarash@gmail.com**\n\n**GitHub : [Link](github.com/arash-mehrzadi)**\n\n**Linkedin : [Link](https://ir.linkedin.com/in/arashmehrzadi)**\n\n**Website : [arashmehrzadi.com](https://www.arashmehrzadi.com/)**\n\n**Import Libraries** \n# **BraTs2020 Data Visualization**\n**In this section, I select a sample data from Brats2020  and visualize it to see the input data**\n**Fluid-attenuated inversion recovery (FLAIR) :**\n\n**FLAIR is an MRI sequence with an inversion recovery set to null fluids. For example, it can be used in brain imaging to suppress cerebrospinal fluid (CSF) effects on the image, so as to bring out the periventricular hyperintense lesions, such as multiple sclerosis (MS) plaques**\n**Manual segmentation that annotated by an expert**\n**T1 weighted images :**\n\n**Magnetic resonance imaging uses the resonance of the protons to generate images. Protons are excited by a radio frequency pulse at an appropriate frequency (Larmor frequency) and then the excess energy is released in the form of a minuscule amount of heat to the surroundings as the spins return to their thermal equilibrium. The magnetization of the proton ensemble goes back to its equilibrium value with an exponential curve characterized by a time constant T1.**\n\n**T1 weighted images can be obtained by setting short repetition time (TR) such as < 750 ms and echo time (TE) such as < 40 ms in conventional spin echo sequences, while in Gradient Echo Sequences they can be obtained by using flip angles of larger than 50o while setting TE values to less than 15 ms.**\n\n**T1 is significantly different between grey matter and white matter and is used when undertaking brain scans. A strong T1 contrast is present between fluid and more solid anatomical structures, making T1 contrast suitable for morphological assessment of the normal or pathological anatomy, e.g., for musculoskeletal applications.**\n**T2 weighted image :**\n\n\n**T2 weighted image (T2WI) is one of the basic pulse sequences in MRI. The sequence weighting highlights differences in the T2 relaxation time of tissues.**\n\n**The amount of T2 decay a tissue experiences depends on multiple factors. Each tissue has an inherent T2 value, but external factors (such as magnetic field inhomogeneity) can decrease the T2 relaxation time. This additional effect is captured in T2*. The refocusing pulse in spin-echo sequences helps to mitigate these extraneous influences on the T2 relaxation time, trying to keep the image T2 weighted rather than T2* weighted.**\n# Data Preprocessing\n**Check the output**\n**concatenate images**\n**Implement Unet**\n![Unet](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n**The u-net is convolutional network architecture for fast and precise segmentation of images. Up to now it has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. It has won the Grand Challenge for Computer-Automated Detection of Caries in Bitewing Radiography at ISBI 2015, and it has won the Cell Tracking Challenge at ISBI 2015 on the two most challenging transmitted light microscopy categories (Phase contrast and DIC microscopy) by a large margin. architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.**\n", "source": "Kaggle", "docid": "Kaggle336", "language": "python", "num_cells": 45, "num_code_cells": 29, "num_md_cells": 16, "len_md_text": 4355, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "Brain Tumor Segmentation Notebook is an MRI sequence with an inversion recovery set to null fluids. it can be used in brain imaging to suppress cerebrospinal fluid (CSF) effects on the image"}, {"source_id": "nurislamsowmik/3d-u-net-on-brats-2020", "name": "3D U-NET on BraTS 2020", "file_name": "nurislamsowmik_3d-u-net-on-brats-2020.ipynb", "html_url": "https://www.kaggle.com/code/nurislamsowmik/3d-u-net-on-brats-2020", "description": "# Problem definiton\n**Segmentation of gliomas in pre-operative MRI scans.**\n\n*Each pixel on image must be labeled:*\n* Pixel is part of a tumor area (1 or 2 or 3) -> can be one of multiple classes / sub-regions\n* Anything else -> pixel is not on a tumor region (0)\n\nThe sub-regions of tumor considered for evaluation are: 1) the \"enhancing tumor\" (ET), 2) the \"tumor core\" (TC), and 3) the \"whole tumor\" (WT)\nThe provided segmentation labels have values of 1 for NCR & NET, 2 for ED, 4 for ET, and 0 for everything else.\n\n\n![Brats official annotations](https://www.med.upenn.edu/cbica/assets/user-content/images/BraTS/brats-tumor-subregions.jpg)\n# Setup env\n# Image data descriptions\n\nAll BraTS multimodal scans are available as  NIfTI files (.nii.gz) -> commonly used medical imaging format to store brain imagin data obtained using MRI and describe different MRI settings \n1. **T1**: T1-weighted, native image, sagittal or axial 2D acquisitions, with 1\u20136 mm slice thickness.\n2. **T1c**: T1-weighted, contrast-enhanced (Gadolinium) image, with 3D acquisition and 1 mm isotropic voxel size for most patients.\n3. **T2**: T2-weighted image, axial 2D acquisition, with 2\u20136 mm slice thickness.\n4. **FLAIR**: T2-weighted FLAIR image, axial, coronal, or sagittal 2D acquisitions, 2\u20136 mm slice thickness.\n\nData were acquired with different clinical protocols and various scanners from multiple (n=19) institutions.\n\nAll the imaging datasets have been segmented manually, by one to four raters, following the same annotation protocol, and their annotations were approved by experienced neuro-radiologists. Annotations comprise the GD-enhancing tumor (ET \u2014 label 4), the peritumoral edema (ED \u2014 label 2), and the necrotic and non-enhancing tumor core (NCR/NET \u2014 label 1), as described both in the BraTS 2012-2013 TMI paper and in the latest BraTS summarizing paper. The provided data are distributed after their pre-processing, i.e., co-registered to the same anatomical template, interpolated to the same resolution (1 mm^3) and skull-stripped.\n\n\n**Show whole nifti data -> print each slice from 3d data**\n**Show segment of tumor for each above slice**\n**Gif representation of slices in 3D volume**\n<img src=\"https://media1.tenor.com/images/15427ffc1399afc3334f12fd27549a95/tenor.gif?itemid=20554734\">\n**Show segments of tumor using different effects**\n# Create model || U-Net: Convolutional Networks for Biomedical Image Segmentation\nhe u-net is convolutional network architecture for fast and precise segmentation of images. Up to now it has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. It has won the Grand Challenge for Computer-Automated Detection of Caries in Bitewing Radiography at ISBI 2015, and it has won the Cell Tracking Challenge at ISBI 2015 on the two most challenging transmitted light microscopy categories (Phase contrast and DIC microscopy) by a large margin\n[more on](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n![official definiton](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n\n# Loss function\n**Dice coefficient**\n, which is essentially a measure of overlap between two samples. This measure ranges from 0 to 1 where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n\n![dice loss](https://wikimedia.org/api/rest_v1/media/math/render/svg/a80a97215e1afc0b222e604af1b2099dc9363d3b)\n\n**As matrices**\n![dice loss](https://www.jeremyjordan.me/content/images/2018/05/intersection-1.png)\n\n[Implementation, (images above) and explanation can be found here](https://www.jeremyjordan.me/semantic-segmentation/)\n**model architecture** <br>\nIf you are about to use 3D U-NET, I suggest to check out this awesome library that I found later, after manual implementation of U-NET [keras-unet-collection](https://pypi.org/project/keras-unet-collection/), which also contains implementation of dice loss, tversky loss and many more!\n# Load data\nLoading all data into memory is not a good idea since the data are too big to fit in.\nSo we will create dataGenerators - load data on the fly as explained [here](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly)\n**Override Keras sequence DataGenerator class**\n**Number of data used**\nfor training / testing / validation\n**Add callback for training process**\n# Train model\nMy best model was trained with 81% accuracy on mean IOU and 65.5% on Dice loss <br>\nI will load this pretrained model instead of training again\n**Visualize the training process**\n# Prediction examples \n# Evaluation\n# Survival prediction\nFull implementation can be found in my another notebook: https://www.kaggle.com/rastislav/mri-brain-tumor-survival-prediction\n", "source": "Kaggle", "docid": "Kaggle489", "language": "python", "num_cells": 42, "num_code_cells": 21, "num_md_cells": 21, "len_md_text": 4865, "summarization_relevance": 0.62, "summarization_confidence": 0.83, "summarization": "pixel on image must be labeled:** Pixel is part of a tumor area (1 or 2 or 3). sub-regions of tumor considered for evaluation are: the \"enhancing tumor\" (ET), 2 for"}], "page_range": 
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import json\n",
    "\n",
    "from kaggle import KaggleApi\n",
    "\n",
    "# with open('/kaggle.json') as f: \n",
    "#     auth = json.load(f)\n",
    "kaggle_api = KaggleApi()\n",
    "# kaggle_api.set_config_value(auth['username'], auth['key'])\n",
    "kaggle_api.authenticate()\n",
    "owner_slug = 'buddhiniw'\n",
    "kernel_slug = 'breast-cancer-prediction'\n",
    "response = kaggle_api.process_response(\n",
    "            kaggle_api.kernel_pull_with_http_info(owner_slug, kernel_slug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'languageNullable': 'python',\n",
       " 'kernelTypeNullable': 'notebook',\n",
       " 'isPrivateNullable': False,\n",
       " 'enableGpuNullable': False,\n",
       " 'enableInternetNullable': False,\n",
       " 'id': 127897,\n",
       " 'ref': 'buddhiniw/breast-cancer-prediction',\n",
       " 'title': 'Breast cancer prediction',\n",
       " 'author': 'Buddhini W',\n",
       " 'slug': 'breast-cancer-prediction',\n",
       " 'lastRunTime': '2016-12-05T07:11:54.87Z',\n",
       " 'language': 'python',\n",
       " 'hasLanguage': True,\n",
       " 'kernelType': 'notebook',\n",
       " 'hasKernelType': True,\n",
       " 'isPrivate': False,\n",
       " 'hasIsPrivate': True,\n",
       " 'enableGpu': False,\n",
       " 'hasEnableGpu': True,\n",
       " 'enableInternet': False,\n",
       " 'hasEnableInternet': True,\n",
       " 'categoryIds': ['healthcare'],\n",
       " 'datasetDataSources': ['datasets/uciml/breast-cancer-wisconsin-data'],\n",
       " 'kernelDataSources': [],\n",
       " 'competitionDataSources': [],\n",
       " 'totalVotes': 315}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sourceNullable', 'languageNullable', 'kernelTypeNullable', 'slugNullable', 'source', 'hasSource', 'language', 'hasLanguage', 'kernelType', 'hasKernelType', 'slug', 'hasSlug'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['blob'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'breast-cancer-prediction'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['blob']['slug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiException",
     "evalue": "(403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Mon, 28 Nov 2022 14:57:34 GMT', 'Access-Control-Allow-Credentials': 'true', 'Set-Cookie': 'ka_sessionid=2ee3e4ba1bfe7c7d1ee8e6c874d9789a; max-age=2626560; path=/, GCLB=CJ_58pGBgt7otQE; path=/; HttpOnly', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Turbolinks-Location': 'https://www.kaggle.com/api/v1/kernels/pull?userName=buddhiniw&kernelSlug=buddhiniw%2Fbreast-cancer-prediction', 'X-Kaggle-MillisecondsElapsed': '48', 'X-Kaggle-RequestId': '55983d5ff208a12cabcd71c93c5dadc4', 'X-Kaggle-ApiVersion': '1.5.12', 'X-Frame-Options': 'SAMEORIGIN', 'Strict-Transport-Security': 'max-age=63072000; includeSubDomains; preload', 'Content-Security-Policy': \"object-src 'none'; script-src 'nonce-L/bzofaBxrAM3dYRsmMKiA==' 'report-sample' 'unsafe-inline' 'unsafe-eval' 'strict-dynamic' https: http:; frame-src 'self' https://www.kaggleusercontent.com https://www.youtube.com/embed/ https://polygraph-cool.github.io https://www.google.com/recaptcha/ https://form.jotform.com https://submit.jotform.us https://submit.jotformpro.com https://submit.jotform.com https://www.docdroid.com https://www.docdroid.net https://kaggle-static.storage.googleapis.com https://kaggle-static-staging.storage.googleapis.com https://kkb-dev.jupyter-proxy.kaggle.net https://kkb-staging.jupyter-proxy.kaggle.net https://kkb-production.jupyter-proxy.kaggle.net https://kkb-dev.firebaseapp.com https://kkb-staging.firebaseapp.com https://kkb-production.firebaseapp.com https://kaggle-metastore-test.firebaseapp.com https://kaggle-metastore.firebaseapp.com https://apis.google.com https://content-sheets.googleapis.com/ https://accounts.google.com/ https://storage.googleapis.com https://docs.google.com https://drive.google.com https://calendar.google.com/; base-uri 'none'; report-uri https://csp.withgoogle.com/csp/kaggle/20201130;\", 'X-Content-Type-Options': 'nosniff', 'Referrer-Policy': 'strict-origin-when-cross-origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"code\":403,\"message\":\"Permission \\u0027notebooks.get\\u0027 was denied\"}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/na/codes/notebook_search_docker/notebookcrawler/kagglecrawler/test.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmc6/home/na/codes/notebook_search_docker/notebookcrawler/kagglecrawler/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m kaggle_api\u001b[39m.\u001b[39mprocess_response(\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmc6/home/na/codes/notebook_search_docker/notebookcrawler/kagglecrawler/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m             kaggle_api\u001b[39m.\u001b[39;49mkernel_pull_with_http_info(owner_slug, kernel_slug))\n",
      "File \u001b[0;32m~/miniconda3/envs/notebooksearch/lib/python3.8/site-packages/kaggle/api/kaggle_api.py:2326\u001b[0m, in \u001b[0;36mKaggleApi.kernel_pull_with_http_info\u001b[0;34m(self, user_name, kernel_slug, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[39m# Authentication setting\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m auth_settings \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mbasicAuth\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_client\u001b[39m.\u001b[39;49mcall_api(\n\u001b[1;32m   2327\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m/kernels/pull\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m   2328\u001b[0m     path_params,\n\u001b[1;32m   2329\u001b[0m     query_params,\n\u001b[1;32m   2330\u001b[0m     header_params,\n\u001b[1;32m   2331\u001b[0m     body\u001b[39m=\u001b[39;49mbody_params,\n\u001b[1;32m   2332\u001b[0m     post_params\u001b[39m=\u001b[39;49mform_params,\n\u001b[1;32m   2333\u001b[0m     files\u001b[39m=\u001b[39;49mlocal_var_files,\n\u001b[1;32m   2334\u001b[0m     response_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mResult\u001b[39;49m\u001b[39m'\u001b[39;49m,  \u001b[39m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m   2335\u001b[0m     auth_settings\u001b[39m=\u001b[39;49mauth_settings,\n\u001b[1;32m   2336\u001b[0m     async_req\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39masync_req\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m   2337\u001b[0m     _return_http_data_only\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_return_http_data_only\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m   2338\u001b[0m     _preload_content\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_preload_content\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m   2339\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_request_timeout\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m   2340\u001b[0m     collection_formats\u001b[39m=\u001b[39;49mcollection_formats)\n",
      "File \u001b[0;32m~/miniconda3/envs/notebooksearch/lib/python3.8/site-packages/kaggle/api_client.py:329\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[39mTo make an async request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__call_api(resource_path, method,\n\u001b[1;32m    330\u001b[0m                            path_params, query_params, header_params,\n\u001b[1;32m    331\u001b[0m                            body, post_params, files,\n\u001b[1;32m    332\u001b[0m                            response_type, auth_settings,\n\u001b[1;32m    333\u001b[0m                            _return_http_data_only, collection_formats,\n\u001b[1;32m    334\u001b[0m                            _preload_content, _request_timeout)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     thread \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mapply_async(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    337\u001b[0m                                    method, path_params, query_params,\n\u001b[1;32m    338\u001b[0m                                    header_params, body,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m                                    collection_formats,\n\u001b[1;32m    343\u001b[0m                                    _preload_content, _request_timeout))\n",
      "File \u001b[0;32m~/miniconda3/envs/notebooksearch/lib/python3.8/site-packages/kaggle/api_client.py:161\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfiguration\u001b[39m.\u001b[39mhost \u001b[39m+\u001b[39m resource_path\n\u001b[1;32m    160\u001b[0m \u001b[39m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m response_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    162\u001b[0m     method, url, query_params\u001b[39m=\u001b[39;49mquery_params, headers\u001b[39m=\u001b[39;49mheader_params,\n\u001b[1;32m    163\u001b[0m     post_params\u001b[39m=\u001b[39;49mpost_params, body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    164\u001b[0m     _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    165\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49m_request_timeout)\n\u001b[1;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_response \u001b[39m=\u001b[39m response_data\n\u001b[1;32m    169\u001b[0m return_data \u001b[39m=\u001b[39m response_data\n",
      "File \u001b[0;32m~/miniconda3/envs/notebooksearch/lib/python3.8/site-packages/kaggle/api_client.py:351\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39m\"\"\"Makes the HTTP request using RESTClient.\"\"\"\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrest_client\u001b[39m.\u001b[39;49mGET(url,\n\u001b[1;32m    352\u001b[0m                                 query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[1;32m    353\u001b[0m                                 _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    354\u001b[0m                                 _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[1;32m    355\u001b[0m                                 headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    356\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    357\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mHEAD(url,\n\u001b[1;32m    358\u001b[0m                                  query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[1;32m    359\u001b[0m                                  _preload_content\u001b[39m=\u001b[39m_preload_content,\n\u001b[1;32m    360\u001b[0m                                  _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[1;32m    361\u001b[0m                                  headers\u001b[39m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/miniconda3/envs/notebooksearch/lib/python3.8/site-packages/kaggle/rest.py:247\u001b[0m, in \u001b[0;36mRESTClientObject.GET\u001b[0;34m(self, url, headers, query_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGET\u001b[39m(\u001b[39mself\u001b[39m, url, headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, query_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _preload_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m         _request_timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url,\n\u001b[1;32m    248\u001b[0m                         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    249\u001b[0m                         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    250\u001b[0m                         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[1;32m    251\u001b[0m                         query_params\u001b[39m=\u001b[39;49mquery_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/notebooksearch/lib/python3.8/site-packages/kaggle/rest.py:241\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    238\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mresponse body: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, r\u001b[39m.\u001b[39mdata)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m299\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mraise\u001b[39;00m ApiException(http_resp\u001b[39m=\u001b[39mr)\n\u001b[1;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "\u001b[0;31mApiException\u001b[0m: (403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Mon, 28 Nov 2022 14:57:34 GMT', 'Access-Control-Allow-Credentials': 'true', 'Set-Cookie': 'ka_sessionid=2ee3e4ba1bfe7c7d1ee8e6c874d9789a; max-age=2626560; path=/, GCLB=CJ_58pGBgt7otQE; path=/; HttpOnly', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Turbolinks-Location': 'https://www.kaggle.com/api/v1/kernels/pull?userName=buddhiniw&kernelSlug=buddhiniw%2Fbreast-cancer-prediction', 'X-Kaggle-MillisecondsElapsed': '48', 'X-Kaggle-RequestId': '55983d5ff208a12cabcd71c93c5dadc4', 'X-Kaggle-ApiVersion': '1.5.12', 'X-Frame-Options': 'SAMEORIGIN', 'Strict-Transport-Security': 'max-age=63072000; includeSubDomains; preload', 'Content-Security-Policy': \"object-src 'none'; script-src 'nonce-L/bzofaBxrAM3dYRsmMKiA==' 'report-sample' 'unsafe-inline' 'unsafe-eval' 'strict-dynamic' https: http:; frame-src 'self' https://www.kaggleusercontent.com https://www.youtube.com/embed/ https://polygraph-cool.github.io https://www.google.com/recaptcha/ https://form.jotform.com https://submit.jotform.us https://submit.jotformpro.com https://submit.jotform.com https://www.docdroid.com https://www.docdroid.net https://kaggle-static.storage.googleapis.com https://kaggle-static-staging.storage.googleapis.com https://kkb-dev.jupyter-proxy.kaggle.net https://kkb-staging.jupyter-proxy.kaggle.net https://kkb-production.jupyter-proxy.kaggle.net https://kkb-dev.firebaseapp.com https://kkb-staging.firebaseapp.com https://kkb-production.firebaseapp.com https://kaggle-metastore-test.firebaseapp.com https://kaggle-metastore.firebaseapp.com https://apis.google.com https://content-sheets.googleapis.com/ https://accounts.google.com/ https://storage.googleapis.com https://docs.google.com https://drive.google.com https://calendar.google.com/; base-uri 'none'; report-uri https://csp.withgoogle.com/csp/kaggle/20201130;\", 'X-Content-Type-Options': 'nosniff', 'Referrer-Policy': 'strict-origin-when-cross-origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"code\":403,\"message\":\"Permission \\u0027notebooks.get\\u0027 was denied\"}\n"
     ]
    }
   ],
   "source": [
    "response = kaggle_api.process_response(\n",
    "            kaggle_api.kernel_pull_with_http_info(owner_slug, kernel_slug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'languageNullable': 'python',\n",
       "  'kernelTypeNullable': 'notebook',\n",
       "  'isPrivateNullable': False,\n",
       "  'enableGpuNullable': False,\n",
       "  'enableInternetNullable': False,\n",
       "  'id': 127897,\n",
       "  'ref': 'buddhiniw/breast-cancer-prediction',\n",
       "  'title': 'Breast cancer prediction',\n",
       "  'author': 'Buddhini W',\n",
       "  'slug': 'breast-cancer-prediction',\n",
       "  'lastRunTime': '2016-12-05T07:11:54.87Z',\n",
       "  'language': 'python',\n",
       "  'hasLanguage': True,\n",
       "  'kernelType': 'notebook',\n",
       "  'hasKernelType': True,\n",
       "  'isPrivate': False,\n",
       "  'hasIsPrivate': True,\n",
       "  'enableGpu': False,\n",
       "  'hasEnableGpu': True,\n",
       "  'enableInternet': False,\n",
       "  'hasEnableInternet': True,\n",
       "  'categoryIds': ['healthcare'],\n",
       "  'datasetDataSources': ['datasets/uciml/breast-cancer-wisconsin-data'],\n",
       "  'kernelDataSources': [],\n",
       "  'competitionDataSources': [],\n",
       "  'totalVotes': 315},\n",
       " 'blob': {'sourceNullable': '{\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"dafef955-4c2c-a871-f1d8-3e0d306393b0\"},\"source\":\"# Using the Wisconsin breast cancer diagnostic data set for predictive analysis\\\\n## Buddhini Waidyawansa (12-03-2016)\\\\nAttribute Information:\\\\n\\\\n - 1) ID number \\\\n - 2) Diagnosis (M = malignant, B = benign) \\\\n \\\\n-3-32.Ten real-valued features are computed for each cell nucleus:\\\\n\\\\n - a) radius (mean of distances from center to points on the perimeter) \\\\n - b) texture (standard deviation of gray-scale values) \\\\n - c) perimeter \\\\n - d) area \\\\n - e) smoothness (local variation in radius lengths) \\\\n - f) compactness (perimeter^2 / area - 1.0) \\\\n - g). concavity (severity of concave portions of the contour) \\\\n - h). concave points (number of concave portions of the contour) \\\\n - i). symmetry \\\\n - j). fractal dimension (\\\\\"coastline approximation\\\\\" - 1)\\\\n\\\\nThe mean, standard error and \\\\\"worst\\\\\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\\\\n\\\\n\\\\nFor this analysis, as a guide to predictive analysis I followed the instructions and discussion on \\\\\"A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)\\\\\" at Analytics Vidhya.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5e26372e-f1bd-b50f-0c1c-33a44306d1f7\"},\"source\":\"#Load Libraries\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"2768ce80-1a7d-ca31-a35f-29cf0ef7fb15\"},\"outputs\":[],\"source\":\"import numpy as np # linear algebra\\\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\\\n\\\\n# keeps the plots in one place. calls image as static pngs\\\\n%matplotlib inline \\\\nimport matplotlib.pyplot as plt # side-stepping mpl backend\\\\nimport matplotlib.gridspec as gridspec # subplots\\\\nimport mpld3 as mpl\\\\n\\\\n#Import models from scikit learn module:\\\\nfrom sklearn.model_selection import train_test_split\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.cross_validation import KFold   #For K-fold cross validation\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\\\\nfrom sklearn import metrics\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"09b9d090-2cba-ad5a-58ce-84208f95dba4\"},\"source\":\"# Load the data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"9180cb22-53d2-6bf2-3a29-99448ab808fb\"},\"outputs\":[],\"source\":\"df = pd.read_csv(\\\\\"../input/data.csv\\\\\",header = 0)\\\\ndf.head()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"e382010d-1d71-b8d6-4a6e-a0abc9e42372\"},\"source\":\"# Clean and prepare data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"f9fd3701-af9d-8d8c-5d0e-e2673d7977fe\"},\"outputs\":[],\"source\":\"df.drop(\\'id\\',axis=1,inplace=True)\\\\ndf.drop(\\'Unnamed: 32\\',axis=1,inplace=True)\\\\n# size of the dataframe\\\\nlen(df)\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"083fe464-8dac-713e-d0a1-46435c0d93fa\"},\"outputs\":[],\"source\":\"df.diagnosis.unique()\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"0882e4c2-3d4d-d4d9-5f49-f36c1b248b93\"},\"outputs\":[],\"source\":\"df[\\'diagnosis\\'] = df[\\'diagnosis\\'].map({\\'M\\':1,\\'B\\':0})\\\\ndf.head()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9308c3e3-af06-6f2d-b4cf-f2dc9a47a881\"},\"source\":\"# Explore data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cfd882cd-1719-4093-934a-539faf665353\"},\"outputs\":[],\"source\":\"df.describe()\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"aa80be8a-4022-038b-d7b7-0789df4ef973\"},\"outputs\":[],\"source\":\"df.describe()\\\\nplt.hist(df[\\'diagnosis\\'])\\\\nplt.title(\\'Diagnosis (M=1 , B=0)\\')\\\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"56b72979-5155-2a99-1b6e-a55cbf72d2a3\"},\"source\":\"### nucleus features vs diagnosis\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"bc36c937-c5d8-8635-480b-777a94571310\"},\"outputs\":[],\"source\":\"features_mean=list(df.columns[1:11])\\\\n# split dataframe into two based on diagnosis\\\\ndfM=df[df[\\'diagnosis\\'] ==1]\\\\ndfB=df[df[\\'diagnosis\\'] ==0]\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"3f3b5e1b-605d-51b4-28c7-c551b5d13a48\"},\"outputs\":[],\"source\":\"#Stack the data\\\\nplt.rcParams.update({\\'font.size\\': 8})\\\\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\\\\naxes = axes.ravel()\\\\nfor idx,ax in enumerate(axes):\\\\n    ax.figure\\\\n    binwidth= (max(df[features_mean[idx]]) - min(df[features_mean[idx]]))/50\\\\n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(df[features_mean[idx]]), max(df[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True, normed = True, label=[\\'M\\',\\'B\\'],color=[\\'r\\',\\'g\\'])\\\\n    ax.legend(loc=\\'upper right\\')\\\\n    ax.set_title(features_mean[idx])\\\\nplt.tight_layout()\\\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"4b8d6133-427b-1ecf-0e24-9ec2afea0a0e\"},\"source\":\"###Observations\\\\n\\\\n1. mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors. \\\\n2. mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. In any of the histograms there are no noticeable large outliers that warrants further cleanup.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ac11039f-0418-3553-9412-ae3d50bef4e4\"},\"source\":\"## Creating a test set and a training set\\\\nSince this data set is not ordered, I am going to do a simple 70:30 split to create a training data set and a test data set.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"1390898b-a338-7395-635f-6e1c216861e6\"},\"outputs\":[],\"source\":\"traindf, testdf = train_test_split(df, test_size = 0.3)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"45dac047-52fb-b847-a521-fa6883ebd5f6\"},\"source\":\"## Model Classification\\\\n\\\\nHere we are going to build a classification model and evaluate its performance using the training set.\\\\n\\\\n\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"780b88d4-523e-b3f8-87dc-17c093663f19\"},\"outputs\":[],\"source\":\"#Generic function for making a classification model and accessing the performance. \\\\n# From AnalyticsVidhya tutorial\\\\ndef classification_model(model, data, predictors, outcome):\\\\n  #Fit the model:\\\\n  model.fit(data[predictors],data[outcome])\\\\n  \\\\n  #Make predictions on training set:\\\\n  predictions = model.predict(data[predictors])\\\\n  \\\\n  #Print accuracy\\\\n  accuracy = metrics.accuracy_score(predictions,data[outcome])\\\\n  print(\\\\\"Accuracy : %s\\\\\" % \\\\\"{0:.3%}\\\\\".format(accuracy))\\\\n\\\\n  #Perform k-fold cross-validation with 5 folds\\\\n  kf = KFold(data.shape[0], n_folds=5)\\\\n  error = []\\\\n  for train, test in kf:\\\\n    # Filter training data\\\\n    train_predictors = (data[predictors].iloc[train,:])\\\\n    \\\\n    # The target we\\'re using to train the algorithm.\\\\n    train_target = data[outcome].iloc[train]\\\\n    \\\\n    # Training the algorithm using the predictors and target.\\\\n    model.fit(train_predictors, train_target)\\\\n    \\\\n    #Record error from each cross-validation run\\\\n    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\\\\n    \\\\n    print(\\\\\"Cross-Validation Score : %s\\\\\" % \\\\\"{0:.3%}\\\\\".format(np.mean(error)))\\\\n    \\\\n  #Fit the model again so that it can be refered outside the function:\\\\n  model.fit(data[predictors],data[outcome]) \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"33763a8f-3917-25c9-b581-d03be3078af2\"},\"source\":\"### Logistic Regression model\\\\n\\\\nLogistic regression is widely used for classification of discrete data. In this case we will use it for binary (1,0) classification.\\\\n\\\\nBased on the observations in the histogram plots, we can reasonably hypothesize that the cancer diagnosis depends on the mean cell radius, mean perimeter, mean area, mean compactness, mean concavity and mean concave points. We can then  perform a logistic regression analysis using those features as follows:\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"23d25895-a0ca-c355-899d-4a838e0d799d\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\',\\'perimeter_mean\\',\\'area_mean\\',\\'compactness_mean\\',\\'concave points_mean\\']\\\\noutcome_var=\\'diagnosis\\'\\\\nmodel=LogisticRegression()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"fcdbefb0-e8b7-990d-e016-5d6de31335a7\"},\"source\":\"The prediction accuracy is reasonable. \\\\nWhat happens if we use just one predictor? \\\\nUse the mean_radius:\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"44cdb278-4b12-5710-f616-db7ccde39775\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\']\\\\nmodel=LogisticRegression()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"24484a16-ccec-e32b-ec46-26f4e7572ad5\"},\"source\":\"This gives a similar prediction accuracy and a cross-validation score.\\\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6572777a-8c9c-1678-3651-65bd79577580\"},\"source\":\"The accuracy of the predictions are good but not great. The cross-validation scores are reasonable. \\\\nCan we do better with another model?\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"2a7ce79c-2557-d306-977c-c44adde27c6b\"},\"source\":\"### Decision Tree Model\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"36acf35e-296e-68fa-ab18-49967554af07\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\',\\'perimeter_mean\\',\\'area_mean\\',\\'compactness_mean\\',\\'concave points_mean\\']\\\\nmodel = DecisionTreeClassifier()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5d88c4cb-d2c7-ca71-5e00-786dd20480b3\"},\"source\":\"Here we are over-fitting the model probably due to the large number of predictors.\\\\nLet use a single predictor, the obvious one is the radius of the cell.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"20ef1197-b886-1517-1282-fe6dbeef15a4\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\']\\\\nmodel = DecisionTreeClassifier()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"936de076-96bd-82d4-25df-284744bc97b5\"},\"source\":\"The accuracy of the prediction is much much better here.  But does it depend on the predictor?\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"7f22bd99-d2e0-b7b8-c100-88a9a0f0c430\"},\"source\":\"Using a single predictor gives a 97% prediction accuracy for this model but the cross-validation score is not that great. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"40ca5482-3e1c-b8c1-2ac2-041897c9d160\"},\"source\":\"### Randome Forest\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"e6edf958-508b-f46c-74a5-f1d8f421900f\"},\"outputs\":[],\"source\":\"# Use all the features of the nucleus\\\\npredictor_var = features_mean\\\\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model, traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6e6fb3f9-6fea-171f-fd18-b29d475c140c\"},\"source\":\"Using all the features improves the prediction accuracy and the cross-validation score is great.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9e9bb834-8871-5d30-5832-0655b005f8e4\"},\"source\":\" An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cc9f7243-0ad7-5d47-9791-a0be8757ff1c\"},\"outputs\":[],\"source\":\"#Create a series with feature importances:\\\\nfeatimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\\\\nprint(featimp)\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cb067a66-eea5-e2e3-ebc3-b18e832680e1\"},\"outputs\":[],\"source\":\"# Using top 5 features\\\\npredictor_var = [\\'concave points_mean\\',\\'area_mean\\',\\'radius_mean\\',\\'perimeter_mean\\',\\'concavity_mean\\',]\\\\nmodel = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"cf8789ef-3898-7c7d-7e46-7526880d71c6\"},\"source\":\"Using the top 5 features only changes the prediction accuracy a bit but I think we get a better result if we use all the predictors.\\\\n\\\\nWhat happens if we use a single predictor as before? Just check.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"3a80b53f-3033-0d57-2d49-9089338b41d6\"},\"outputs\":[],\"source\":\"predictor_var =  [\\'radius_mean\\']\\\\nmodel = RandomForestClassifier(n_estimators=100)\\\\nclassification_model(model, traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"d28e57c8-4e19-1f69-f7e2-b11a3c392d84\"},\"source\":\"This gives a better prediction accuracy too but the cross-validation is not great.\\\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"48ad851d-8bb1-386d-30cb-675acf93d078\"},\"source\":\"## Using on the test data set\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"0666b20a-7359-2cf6-2792-08103d78afca\"},\"outputs\":[],\"source\":\"# Use all the features of the nucleus\\\\npredictor_var = features_mean\\\\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model, testdf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"4d63d451-380e-1dde-b3da-048afc63d019\"},\"source\":\"The prediction accuracy for the test data set using the above Random Forest model is 95%!\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0310c568-10fc-b19d-b2ee-c87a8f3f3280\"},\"source\":\"## Conclusion\\\\n\\\\nThe best model to be used for diagnosing breast cancer as found in this analysis is the Random Forest model with the top 5 predictors, \\'concave points_mean\\',\\'area_mean\\',\\'radius_mean\\',\\'perimeter_mean\\',\\'concavity_mean\\'. It gives a prediction accuracy of ~95% and a cross-validation score ~ 93% for the test data set.\\\\n\\\\n\\\\nI will see if I can improve this more by tweaking the model further and trying out other models in a later version of this analysis.\"}],\"metadata\":{\"_change_revision\":0,\"_is_fork\":false,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"file_extension\":\".py\",\"mimetype\":\"text/x-python\",\"name\":\"python\",\"nbconvert_exporter\":\"python\",\"pygments_lexer\":\"ipython3\",\"version\":\"3.5.2\"}},\"nbformat\":4,\"nbformat_minor\":0}',\n",
       "  'languageNullable': 'python',\n",
       "  'kernelTypeNullable': 'notebook',\n",
       "  'slugNullable': 'breast-cancer-prediction',\n",
       "  'source': '{\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"dafef955-4c2c-a871-f1d8-3e0d306393b0\"},\"source\":\"# Using the Wisconsin breast cancer diagnostic data set for predictive analysis\\\\n## Buddhini Waidyawansa (12-03-2016)\\\\nAttribute Information:\\\\n\\\\n - 1) ID number \\\\n - 2) Diagnosis (M = malignant, B = benign) \\\\n \\\\n-3-32.Ten real-valued features are computed for each cell nucleus:\\\\n\\\\n - a) radius (mean of distances from center to points on the perimeter) \\\\n - b) texture (standard deviation of gray-scale values) \\\\n - c) perimeter \\\\n - d) area \\\\n - e) smoothness (local variation in radius lengths) \\\\n - f) compactness (perimeter^2 / area - 1.0) \\\\n - g). concavity (severity of concave portions of the contour) \\\\n - h). concave points (number of concave portions of the contour) \\\\n - i). symmetry \\\\n - j). fractal dimension (\\\\\"coastline approximation\\\\\" - 1)\\\\n\\\\nThe mean, standard error and \\\\\"worst\\\\\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\\\\n\\\\n\\\\nFor this analysis, as a guide to predictive analysis I followed the instructions and discussion on \\\\\"A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)\\\\\" at Analytics Vidhya.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5e26372e-f1bd-b50f-0c1c-33a44306d1f7\"},\"source\":\"#Load Libraries\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"2768ce80-1a7d-ca31-a35f-29cf0ef7fb15\"},\"outputs\":[],\"source\":\"import numpy as np # linear algebra\\\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\\\n\\\\n# keeps the plots in one place. calls image as static pngs\\\\n%matplotlib inline \\\\nimport matplotlib.pyplot as plt # side-stepping mpl backend\\\\nimport matplotlib.gridspec as gridspec # subplots\\\\nimport mpld3 as mpl\\\\n\\\\n#Import models from scikit learn module:\\\\nfrom sklearn.model_selection import train_test_split\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.cross_validation import KFold   #For K-fold cross validation\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\\\\nfrom sklearn import metrics\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"09b9d090-2cba-ad5a-58ce-84208f95dba4\"},\"source\":\"# Load the data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"9180cb22-53d2-6bf2-3a29-99448ab808fb\"},\"outputs\":[],\"source\":\"df = pd.read_csv(\\\\\"../input/data.csv\\\\\",header = 0)\\\\ndf.head()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"e382010d-1d71-b8d6-4a6e-a0abc9e42372\"},\"source\":\"# Clean and prepare data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"f9fd3701-af9d-8d8c-5d0e-e2673d7977fe\"},\"outputs\":[],\"source\":\"df.drop(\\'id\\',axis=1,inplace=True)\\\\ndf.drop(\\'Unnamed: 32\\',axis=1,inplace=True)\\\\n# size of the dataframe\\\\nlen(df)\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"083fe464-8dac-713e-d0a1-46435c0d93fa\"},\"outputs\":[],\"source\":\"df.diagnosis.unique()\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"0882e4c2-3d4d-d4d9-5f49-f36c1b248b93\"},\"outputs\":[],\"source\":\"df[\\'diagnosis\\'] = df[\\'diagnosis\\'].map({\\'M\\':1,\\'B\\':0})\\\\ndf.head()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9308c3e3-af06-6f2d-b4cf-f2dc9a47a881\"},\"source\":\"# Explore data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cfd882cd-1719-4093-934a-539faf665353\"},\"outputs\":[],\"source\":\"df.describe()\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"aa80be8a-4022-038b-d7b7-0789df4ef973\"},\"outputs\":[],\"source\":\"df.describe()\\\\nplt.hist(df[\\'diagnosis\\'])\\\\nplt.title(\\'Diagnosis (M=1 , B=0)\\')\\\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"56b72979-5155-2a99-1b6e-a55cbf72d2a3\"},\"source\":\"### nucleus features vs diagnosis\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"bc36c937-c5d8-8635-480b-777a94571310\"},\"outputs\":[],\"source\":\"features_mean=list(df.columns[1:11])\\\\n# split dataframe into two based on diagnosis\\\\ndfM=df[df[\\'diagnosis\\'] ==1]\\\\ndfB=df[df[\\'diagnosis\\'] ==0]\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"3f3b5e1b-605d-51b4-28c7-c551b5d13a48\"},\"outputs\":[],\"source\":\"#Stack the data\\\\nplt.rcParams.update({\\'font.size\\': 8})\\\\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\\\\naxes = axes.ravel()\\\\nfor idx,ax in enumerate(axes):\\\\n    ax.figure\\\\n    binwidth= (max(df[features_mean[idx]]) - min(df[features_mean[idx]]))/50\\\\n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(df[features_mean[idx]]), max(df[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True, normed = True, label=[\\'M\\',\\'B\\'],color=[\\'r\\',\\'g\\'])\\\\n    ax.legend(loc=\\'upper right\\')\\\\n    ax.set_title(features_mean[idx])\\\\nplt.tight_layout()\\\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"4b8d6133-427b-1ecf-0e24-9ec2afea0a0e\"},\"source\":\"###Observations\\\\n\\\\n1. mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors. \\\\n2. mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. In any of the histograms there are no noticeable large outliers that warrants further cleanup.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ac11039f-0418-3553-9412-ae3d50bef4e4\"},\"source\":\"## Creating a test set and a training set\\\\nSince this data set is not ordered, I am going to do a simple 70:30 split to create a training data set and a test data set.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"1390898b-a338-7395-635f-6e1c216861e6\"},\"outputs\":[],\"source\":\"traindf, testdf = train_test_split(df, test_size = 0.3)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"45dac047-52fb-b847-a521-fa6883ebd5f6\"},\"source\":\"## Model Classification\\\\n\\\\nHere we are going to build a classification model and evaluate its performance using the training set.\\\\n\\\\n\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"780b88d4-523e-b3f8-87dc-17c093663f19\"},\"outputs\":[],\"source\":\"#Generic function for making a classification model and accessing the performance. \\\\n# From AnalyticsVidhya tutorial\\\\ndef classification_model(model, data, predictors, outcome):\\\\n  #Fit the model:\\\\n  model.fit(data[predictors],data[outcome])\\\\n  \\\\n  #Make predictions on training set:\\\\n  predictions = model.predict(data[predictors])\\\\n  \\\\n  #Print accuracy\\\\n  accuracy = metrics.accuracy_score(predictions,data[outcome])\\\\n  print(\\\\\"Accuracy : %s\\\\\" % \\\\\"{0:.3%}\\\\\".format(accuracy))\\\\n\\\\n  #Perform k-fold cross-validation with 5 folds\\\\n  kf = KFold(data.shape[0], n_folds=5)\\\\n  error = []\\\\n  for train, test in kf:\\\\n    # Filter training data\\\\n    train_predictors = (data[predictors].iloc[train,:])\\\\n    \\\\n    # The target we\\'re using to train the algorithm.\\\\n    train_target = data[outcome].iloc[train]\\\\n    \\\\n    # Training the algorithm using the predictors and target.\\\\n    model.fit(train_predictors, train_target)\\\\n    \\\\n    #Record error from each cross-validation run\\\\n    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\\\\n    \\\\n    print(\\\\\"Cross-Validation Score : %s\\\\\" % \\\\\"{0:.3%}\\\\\".format(np.mean(error)))\\\\n    \\\\n  #Fit the model again so that it can be refered outside the function:\\\\n  model.fit(data[predictors],data[outcome]) \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"33763a8f-3917-25c9-b581-d03be3078af2\"},\"source\":\"### Logistic Regression model\\\\n\\\\nLogistic regression is widely used for classification of discrete data. In this case we will use it for binary (1,0) classification.\\\\n\\\\nBased on the observations in the histogram plots, we can reasonably hypothesize that the cancer diagnosis depends on the mean cell radius, mean perimeter, mean area, mean compactness, mean concavity and mean concave points. We can then  perform a logistic regression analysis using those features as follows:\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"23d25895-a0ca-c355-899d-4a838e0d799d\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\',\\'perimeter_mean\\',\\'area_mean\\',\\'compactness_mean\\',\\'concave points_mean\\']\\\\noutcome_var=\\'diagnosis\\'\\\\nmodel=LogisticRegression()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"fcdbefb0-e8b7-990d-e016-5d6de31335a7\"},\"source\":\"The prediction accuracy is reasonable. \\\\nWhat happens if we use just one predictor? \\\\nUse the mean_radius:\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"44cdb278-4b12-5710-f616-db7ccde39775\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\']\\\\nmodel=LogisticRegression()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"24484a16-ccec-e32b-ec46-26f4e7572ad5\"},\"source\":\"This gives a similar prediction accuracy and a cross-validation score.\\\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6572777a-8c9c-1678-3651-65bd79577580\"},\"source\":\"The accuracy of the predictions are good but not great. The cross-validation scores are reasonable. \\\\nCan we do better with another model?\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"2a7ce79c-2557-d306-977c-c44adde27c6b\"},\"source\":\"### Decision Tree Model\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"36acf35e-296e-68fa-ab18-49967554af07\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\',\\'perimeter_mean\\',\\'area_mean\\',\\'compactness_mean\\',\\'concave points_mean\\']\\\\nmodel = DecisionTreeClassifier()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5d88c4cb-d2c7-ca71-5e00-786dd20480b3\"},\"source\":\"Here we are over-fitting the model probably due to the large number of predictors.\\\\nLet use a single predictor, the obvious one is the radius of the cell.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"20ef1197-b886-1517-1282-fe6dbeef15a4\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\']\\\\nmodel = DecisionTreeClassifier()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"936de076-96bd-82d4-25df-284744bc97b5\"},\"source\":\"The accuracy of the prediction is much much better here.  But does it depend on the predictor?\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"7f22bd99-d2e0-b7b8-c100-88a9a0f0c430\"},\"source\":\"Using a single predictor gives a 97% prediction accuracy for this model but the cross-validation score is not that great. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"40ca5482-3e1c-b8c1-2ac2-041897c9d160\"},\"source\":\"### Randome Forest\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"e6edf958-508b-f46c-74a5-f1d8f421900f\"},\"outputs\":[],\"source\":\"# Use all the features of the nucleus\\\\npredictor_var = features_mean\\\\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model, traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6e6fb3f9-6fea-171f-fd18-b29d475c140c\"},\"source\":\"Using all the features improves the prediction accuracy and the cross-validation score is great.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9e9bb834-8871-5d30-5832-0655b005f8e4\"},\"source\":\" An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cc9f7243-0ad7-5d47-9791-a0be8757ff1c\"},\"outputs\":[],\"source\":\"#Create a series with feature importances:\\\\nfeatimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\\\\nprint(featimp)\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cb067a66-eea5-e2e3-ebc3-b18e832680e1\"},\"outputs\":[],\"source\":\"# Using top 5 features\\\\npredictor_var = [\\'concave points_mean\\',\\'area_mean\\',\\'radius_mean\\',\\'perimeter_mean\\',\\'concavity_mean\\',]\\\\nmodel = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"cf8789ef-3898-7c7d-7e46-7526880d71c6\"},\"source\":\"Using the top 5 features only changes the prediction accuracy a bit but I think we get a better result if we use all the predictors.\\\\n\\\\nWhat happens if we use a single predictor as before? Just check.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"3a80b53f-3033-0d57-2d49-9089338b41d6\"},\"outputs\":[],\"source\":\"predictor_var =  [\\'radius_mean\\']\\\\nmodel = RandomForestClassifier(n_estimators=100)\\\\nclassification_model(model, traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"d28e57c8-4e19-1f69-f7e2-b11a3c392d84\"},\"source\":\"This gives a better prediction accuracy too but the cross-validation is not great.\\\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"48ad851d-8bb1-386d-30cb-675acf93d078\"},\"source\":\"## Using on the test data set\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"0666b20a-7359-2cf6-2792-08103d78afca\"},\"outputs\":[],\"source\":\"# Use all the features of the nucleus\\\\npredictor_var = features_mean\\\\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model, testdf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"4d63d451-380e-1dde-b3da-048afc63d019\"},\"source\":\"The prediction accuracy for the test data set using the above Random Forest model is 95%!\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0310c568-10fc-b19d-b2ee-c87a8f3f3280\"},\"source\":\"## Conclusion\\\\n\\\\nThe best model to be used for diagnosing breast cancer as found in this analysis is the Random Forest model with the top 5 predictors, \\'concave points_mean\\',\\'area_mean\\',\\'radius_mean\\',\\'perimeter_mean\\',\\'concavity_mean\\'. It gives a prediction accuracy of ~95% and a cross-validation score ~ 93% for the test data set.\\\\n\\\\n\\\\nI will see if I can improve this more by tweaking the model further and trying out other models in a later version of this analysis.\"}],\"metadata\":{\"_change_revision\":0,\"_is_fork\":false,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"file_extension\":\".py\",\"mimetype\":\"text/x-python\",\"name\":\"python\",\"nbconvert_exporter\":\"python\",\"pygments_lexer\":\"ipython3\",\"version\":\"3.5.2\"}},\"nbformat\":4,\"nbformat_minor\":0}',\n",
       "  'hasSource': True,\n",
       "  'language': 'python',\n",
       "  'hasLanguage': True,\n",
       "  'kernelType': 'notebook',\n",
       "  'hasKernelType': True,\n",
       "  'slug': 'breast-cancer-prediction',\n",
       "  'hasSlug': True}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sourceNullable': '{\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"dafef955-4c2c-a871-f1d8-3e0d306393b0\"},\"source\":\"# Using the Wisconsin breast cancer diagnostic data set for predictive analysis\\\\n## Buddhini Waidyawansa (12-03-2016)\\\\nAttribute Information:\\\\n\\\\n - 1) ID number \\\\n - 2) Diagnosis (M = malignant, B = benign) \\\\n \\\\n-3-32.Ten real-valued features are computed for each cell nucleus:\\\\n\\\\n - a) radius (mean of distances from center to points on the perimeter) \\\\n - b) texture (standard deviation of gray-scale values) \\\\n - c) perimeter \\\\n - d) area \\\\n - e) smoothness (local variation in radius lengths) \\\\n - f) compactness (perimeter^2 / area - 1.0) \\\\n - g). concavity (severity of concave portions of the contour) \\\\n - h). concave points (number of concave portions of the contour) \\\\n - i). symmetry \\\\n - j). fractal dimension (\\\\\"coastline approximation\\\\\" - 1)\\\\n\\\\nThe mean, standard error and \\\\\"worst\\\\\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\\\\n\\\\n\\\\nFor this analysis, as a guide to predictive analysis I followed the instructions and discussion on \\\\\"A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)\\\\\" at Analytics Vidhya.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5e26372e-f1bd-b50f-0c1c-33a44306d1f7\"},\"source\":\"#Load Libraries\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"2768ce80-1a7d-ca31-a35f-29cf0ef7fb15\"},\"outputs\":[],\"source\":\"import numpy as np # linear algebra\\\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\\\n\\\\n# keeps the plots in one place. calls image as static pngs\\\\n%matplotlib inline \\\\nimport matplotlib.pyplot as plt # side-stepping mpl backend\\\\nimport matplotlib.gridspec as gridspec # subplots\\\\nimport mpld3 as mpl\\\\n\\\\n#Import models from scikit learn module:\\\\nfrom sklearn.model_selection import train_test_split\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.cross_validation import KFold   #For K-fold cross validation\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\\\\nfrom sklearn import metrics\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"09b9d090-2cba-ad5a-58ce-84208f95dba4\"},\"source\":\"# Load the data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"9180cb22-53d2-6bf2-3a29-99448ab808fb\"},\"outputs\":[],\"source\":\"df = pd.read_csv(\\\\\"../input/data.csv\\\\\",header = 0)\\\\ndf.head()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"e382010d-1d71-b8d6-4a6e-a0abc9e42372\"},\"source\":\"# Clean and prepare data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"f9fd3701-af9d-8d8c-5d0e-e2673d7977fe\"},\"outputs\":[],\"source\":\"df.drop(\\'id\\',axis=1,inplace=True)\\\\ndf.drop(\\'Unnamed: 32\\',axis=1,inplace=True)\\\\n# size of the dataframe\\\\nlen(df)\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"083fe464-8dac-713e-d0a1-46435c0d93fa\"},\"outputs\":[],\"source\":\"df.diagnosis.unique()\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"0882e4c2-3d4d-d4d9-5f49-f36c1b248b93\"},\"outputs\":[],\"source\":\"df[\\'diagnosis\\'] = df[\\'diagnosis\\'].map({\\'M\\':1,\\'B\\':0})\\\\ndf.head()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9308c3e3-af06-6f2d-b4cf-f2dc9a47a881\"},\"source\":\"# Explore data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cfd882cd-1719-4093-934a-539faf665353\"},\"outputs\":[],\"source\":\"df.describe()\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"aa80be8a-4022-038b-d7b7-0789df4ef973\"},\"outputs\":[],\"source\":\"df.describe()\\\\nplt.hist(df[\\'diagnosis\\'])\\\\nplt.title(\\'Diagnosis (M=1 , B=0)\\')\\\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"56b72979-5155-2a99-1b6e-a55cbf72d2a3\"},\"source\":\"### nucleus features vs diagnosis\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"bc36c937-c5d8-8635-480b-777a94571310\"},\"outputs\":[],\"source\":\"features_mean=list(df.columns[1:11])\\\\n# split dataframe into two based on diagnosis\\\\ndfM=df[df[\\'diagnosis\\'] ==1]\\\\ndfB=df[df[\\'diagnosis\\'] ==0]\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"3f3b5e1b-605d-51b4-28c7-c551b5d13a48\"},\"outputs\":[],\"source\":\"#Stack the data\\\\nplt.rcParams.update({\\'font.size\\': 8})\\\\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\\\\naxes = axes.ravel()\\\\nfor idx,ax in enumerate(axes):\\\\n    ax.figure\\\\n    binwidth= (max(df[features_mean[idx]]) - min(df[features_mean[idx]]))/50\\\\n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(df[features_mean[idx]]), max(df[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True, normed = True, label=[\\'M\\',\\'B\\'],color=[\\'r\\',\\'g\\'])\\\\n    ax.legend(loc=\\'upper right\\')\\\\n    ax.set_title(features_mean[idx])\\\\nplt.tight_layout()\\\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"4b8d6133-427b-1ecf-0e24-9ec2afea0a0e\"},\"source\":\"###Observations\\\\n\\\\n1. mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors. \\\\n2. mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. In any of the histograms there are no noticeable large outliers that warrants further cleanup.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ac11039f-0418-3553-9412-ae3d50bef4e4\"},\"source\":\"## Creating a test set and a training set\\\\nSince this data set is not ordered, I am going to do a simple 70:30 split to create a training data set and a test data set.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"1390898b-a338-7395-635f-6e1c216861e6\"},\"outputs\":[],\"source\":\"traindf, testdf = train_test_split(df, test_size = 0.3)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"45dac047-52fb-b847-a521-fa6883ebd5f6\"},\"source\":\"## Model Classification\\\\n\\\\nHere we are going to build a classification model and evaluate its performance using the training set.\\\\n\\\\n\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"780b88d4-523e-b3f8-87dc-17c093663f19\"},\"outputs\":[],\"source\":\"#Generic function for making a classification model and accessing the performance. \\\\n# From AnalyticsVidhya tutorial\\\\ndef classification_model(model, data, predictors, outcome):\\\\n  #Fit the model:\\\\n  model.fit(data[predictors],data[outcome])\\\\n  \\\\n  #Make predictions on training set:\\\\n  predictions = model.predict(data[predictors])\\\\n  \\\\n  #Print accuracy\\\\n  accuracy = metrics.accuracy_score(predictions,data[outcome])\\\\n  print(\\\\\"Accuracy : %s\\\\\" % \\\\\"{0:.3%}\\\\\".format(accuracy))\\\\n\\\\n  #Perform k-fold cross-validation with 5 folds\\\\n  kf = KFold(data.shape[0], n_folds=5)\\\\n  error = []\\\\n  for train, test in kf:\\\\n    # Filter training data\\\\n    train_predictors = (data[predictors].iloc[train,:])\\\\n    \\\\n    # The target we\\'re using to train the algorithm.\\\\n    train_target = data[outcome].iloc[train]\\\\n    \\\\n    # Training the algorithm using the predictors and target.\\\\n    model.fit(train_predictors, train_target)\\\\n    \\\\n    #Record error from each cross-validation run\\\\n    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\\\\n    \\\\n    print(\\\\\"Cross-Validation Score : %s\\\\\" % \\\\\"{0:.3%}\\\\\".format(np.mean(error)))\\\\n    \\\\n  #Fit the model again so that it can be refered outside the function:\\\\n  model.fit(data[predictors],data[outcome]) \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"33763a8f-3917-25c9-b581-d03be3078af2\"},\"source\":\"### Logistic Regression model\\\\n\\\\nLogistic regression is widely used for classification of discrete data. In this case we will use it for binary (1,0) classification.\\\\n\\\\nBased on the observations in the histogram plots, we can reasonably hypothesize that the cancer diagnosis depends on the mean cell radius, mean perimeter, mean area, mean compactness, mean concavity and mean concave points. We can then  perform a logistic regression analysis using those features as follows:\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"23d25895-a0ca-c355-899d-4a838e0d799d\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\',\\'perimeter_mean\\',\\'area_mean\\',\\'compactness_mean\\',\\'concave points_mean\\']\\\\noutcome_var=\\'diagnosis\\'\\\\nmodel=LogisticRegression()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"fcdbefb0-e8b7-990d-e016-5d6de31335a7\"},\"source\":\"The prediction accuracy is reasonable. \\\\nWhat happens if we use just one predictor? \\\\nUse the mean_radius:\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"44cdb278-4b12-5710-f616-db7ccde39775\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\']\\\\nmodel=LogisticRegression()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"24484a16-ccec-e32b-ec46-26f4e7572ad5\"},\"source\":\"This gives a similar prediction accuracy and a cross-validation score.\\\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6572777a-8c9c-1678-3651-65bd79577580\"},\"source\":\"The accuracy of the predictions are good but not great. The cross-validation scores are reasonable. \\\\nCan we do better with another model?\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"2a7ce79c-2557-d306-977c-c44adde27c6b\"},\"source\":\"### Decision Tree Model\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"36acf35e-296e-68fa-ab18-49967554af07\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\',\\'perimeter_mean\\',\\'area_mean\\',\\'compactness_mean\\',\\'concave points_mean\\']\\\\nmodel = DecisionTreeClassifier()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5d88c4cb-d2c7-ca71-5e00-786dd20480b3\"},\"source\":\"Here we are over-fitting the model probably due to the large number of predictors.\\\\nLet use a single predictor, the obvious one is the radius of the cell.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"20ef1197-b886-1517-1282-fe6dbeef15a4\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\']\\\\nmodel = DecisionTreeClassifier()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"936de076-96bd-82d4-25df-284744bc97b5\"},\"source\":\"The accuracy of the prediction is much much better here.  But does it depend on the predictor?\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"7f22bd99-d2e0-b7b8-c100-88a9a0f0c430\"},\"source\":\"Using a single predictor gives a 97% prediction accuracy for this model but the cross-validation score is not that great. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"40ca5482-3e1c-b8c1-2ac2-041897c9d160\"},\"source\":\"### Randome Forest\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"e6edf958-508b-f46c-74a5-f1d8f421900f\"},\"outputs\":[],\"source\":\"# Use all the features of the nucleus\\\\npredictor_var = features_mean\\\\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model, traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6e6fb3f9-6fea-171f-fd18-b29d475c140c\"},\"source\":\"Using all the features improves the prediction accuracy and the cross-validation score is great.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9e9bb834-8871-5d30-5832-0655b005f8e4\"},\"source\":\" An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cc9f7243-0ad7-5d47-9791-a0be8757ff1c\"},\"outputs\":[],\"source\":\"#Create a series with feature importances:\\\\nfeatimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\\\\nprint(featimp)\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cb067a66-eea5-e2e3-ebc3-b18e832680e1\"},\"outputs\":[],\"source\":\"# Using top 5 features\\\\npredictor_var = [\\'concave points_mean\\',\\'area_mean\\',\\'radius_mean\\',\\'perimeter_mean\\',\\'concavity_mean\\',]\\\\nmodel = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"cf8789ef-3898-7c7d-7e46-7526880d71c6\"},\"source\":\"Using the top 5 features only changes the prediction accuracy a bit but I think we get a better result if we use all the predictors.\\\\n\\\\nWhat happens if we use a single predictor as before? Just check.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"3a80b53f-3033-0d57-2d49-9089338b41d6\"},\"outputs\":[],\"source\":\"predictor_var =  [\\'radius_mean\\']\\\\nmodel = RandomForestClassifier(n_estimators=100)\\\\nclassification_model(model, traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"d28e57c8-4e19-1f69-f7e2-b11a3c392d84\"},\"source\":\"This gives a better prediction accuracy too but the cross-validation is not great.\\\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"48ad851d-8bb1-386d-30cb-675acf93d078\"},\"source\":\"## Using on the test data set\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"0666b20a-7359-2cf6-2792-08103d78afca\"},\"outputs\":[],\"source\":\"# Use all the features of the nucleus\\\\npredictor_var = features_mean\\\\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model, testdf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"4d63d451-380e-1dde-b3da-048afc63d019\"},\"source\":\"The prediction accuracy for the test data set using the above Random Forest model is 95%!\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0310c568-10fc-b19d-b2ee-c87a8f3f3280\"},\"source\":\"## Conclusion\\\\n\\\\nThe best model to be used for diagnosing breast cancer as found in this analysis is the Random Forest model with the top 5 predictors, \\'concave points_mean\\',\\'area_mean\\',\\'radius_mean\\',\\'perimeter_mean\\',\\'concavity_mean\\'. It gives a prediction accuracy of ~95% and a cross-validation score ~ 93% for the test data set.\\\\n\\\\n\\\\nI will see if I can improve this more by tweaking the model further and trying out other models in a later version of this analysis.\"}],\"metadata\":{\"_change_revision\":0,\"_is_fork\":false,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"file_extension\":\".py\",\"mimetype\":\"text/x-python\",\"name\":\"python\",\"nbconvert_exporter\":\"python\",\"pygments_lexer\":\"ipython3\",\"version\":\"3.5.2\"}},\"nbformat\":4,\"nbformat_minor\":0}',\n",
       " 'languageNullable': 'python',\n",
       " 'kernelTypeNullable': 'notebook',\n",
       " 'slugNullable': 'breast-cancer-prediction',\n",
       " 'source': '{\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"dafef955-4c2c-a871-f1d8-3e0d306393b0\"},\"source\":\"# Using the Wisconsin breast cancer diagnostic data set for predictive analysis\\\\n## Buddhini Waidyawansa (12-03-2016)\\\\nAttribute Information:\\\\n\\\\n - 1) ID number \\\\n - 2) Diagnosis (M = malignant, B = benign) \\\\n \\\\n-3-32.Ten real-valued features are computed for each cell nucleus:\\\\n\\\\n - a) radius (mean of distances from center to points on the perimeter) \\\\n - b) texture (standard deviation of gray-scale values) \\\\n - c) perimeter \\\\n - d) area \\\\n - e) smoothness (local variation in radius lengths) \\\\n - f) compactness (perimeter^2 / area - 1.0) \\\\n - g). concavity (severity of concave portions of the contour) \\\\n - h). concave points (number of concave portions of the contour) \\\\n - i). symmetry \\\\n - j). fractal dimension (\\\\\"coastline approximation\\\\\" - 1)\\\\n\\\\nThe mean, standard error and \\\\\"worst\\\\\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\\\\n\\\\n\\\\nFor this analysis, as a guide to predictive analysis I followed the instructions and discussion on \\\\\"A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)\\\\\" at Analytics Vidhya.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5e26372e-f1bd-b50f-0c1c-33a44306d1f7\"},\"source\":\"#Load Libraries\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"2768ce80-1a7d-ca31-a35f-29cf0ef7fb15\"},\"outputs\":[],\"source\":\"import numpy as np # linear algebra\\\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\\\n\\\\n# keeps the plots in one place. calls image as static pngs\\\\n%matplotlib inline \\\\nimport matplotlib.pyplot as plt # side-stepping mpl backend\\\\nimport matplotlib.gridspec as gridspec # subplots\\\\nimport mpld3 as mpl\\\\n\\\\n#Import models from scikit learn module:\\\\nfrom sklearn.model_selection import train_test_split\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.cross_validation import KFold   #For K-fold cross validation\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\\\\nfrom sklearn import metrics\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"09b9d090-2cba-ad5a-58ce-84208f95dba4\"},\"source\":\"# Load the data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"9180cb22-53d2-6bf2-3a29-99448ab808fb\"},\"outputs\":[],\"source\":\"df = pd.read_csv(\\\\\"../input/data.csv\\\\\",header = 0)\\\\ndf.head()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"e382010d-1d71-b8d6-4a6e-a0abc9e42372\"},\"source\":\"# Clean and prepare data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"f9fd3701-af9d-8d8c-5d0e-e2673d7977fe\"},\"outputs\":[],\"source\":\"df.drop(\\'id\\',axis=1,inplace=True)\\\\ndf.drop(\\'Unnamed: 32\\',axis=1,inplace=True)\\\\n# size of the dataframe\\\\nlen(df)\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"083fe464-8dac-713e-d0a1-46435c0d93fa\"},\"outputs\":[],\"source\":\"df.diagnosis.unique()\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"0882e4c2-3d4d-d4d9-5f49-f36c1b248b93\"},\"outputs\":[],\"source\":\"df[\\'diagnosis\\'] = df[\\'diagnosis\\'].map({\\'M\\':1,\\'B\\':0})\\\\ndf.head()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9308c3e3-af06-6f2d-b4cf-f2dc9a47a881\"},\"source\":\"# Explore data\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cfd882cd-1719-4093-934a-539faf665353\"},\"outputs\":[],\"source\":\"df.describe()\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"aa80be8a-4022-038b-d7b7-0789df4ef973\"},\"outputs\":[],\"source\":\"df.describe()\\\\nplt.hist(df[\\'diagnosis\\'])\\\\nplt.title(\\'Diagnosis (M=1 , B=0)\\')\\\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"56b72979-5155-2a99-1b6e-a55cbf72d2a3\"},\"source\":\"### nucleus features vs diagnosis\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"bc36c937-c5d8-8635-480b-777a94571310\"},\"outputs\":[],\"source\":\"features_mean=list(df.columns[1:11])\\\\n# split dataframe into two based on diagnosis\\\\ndfM=df[df[\\'diagnosis\\'] ==1]\\\\ndfB=df[df[\\'diagnosis\\'] ==0]\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"3f3b5e1b-605d-51b4-28c7-c551b5d13a48\"},\"outputs\":[],\"source\":\"#Stack the data\\\\nplt.rcParams.update({\\'font.size\\': 8})\\\\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\\\\naxes = axes.ravel()\\\\nfor idx,ax in enumerate(axes):\\\\n    ax.figure\\\\n    binwidth= (max(df[features_mean[idx]]) - min(df[features_mean[idx]]))/50\\\\n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(df[features_mean[idx]]), max(df[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True, normed = True, label=[\\'M\\',\\'B\\'],color=[\\'r\\',\\'g\\'])\\\\n    ax.legend(loc=\\'upper right\\')\\\\n    ax.set_title(features_mean[idx])\\\\nplt.tight_layout()\\\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"4b8d6133-427b-1ecf-0e24-9ec2afea0a0e\"},\"source\":\"###Observations\\\\n\\\\n1. mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors. \\\\n2. mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. In any of the histograms there are no noticeable large outliers that warrants further cleanup.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ac11039f-0418-3553-9412-ae3d50bef4e4\"},\"source\":\"## Creating a test set and a training set\\\\nSince this data set is not ordered, I am going to do a simple 70:30 split to create a training data set and a test data set.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"1390898b-a338-7395-635f-6e1c216861e6\"},\"outputs\":[],\"source\":\"traindf, testdf = train_test_split(df, test_size = 0.3)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"45dac047-52fb-b847-a521-fa6883ebd5f6\"},\"source\":\"## Model Classification\\\\n\\\\nHere we are going to build a classification model and evaluate its performance using the training set.\\\\n\\\\n\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"780b88d4-523e-b3f8-87dc-17c093663f19\"},\"outputs\":[],\"source\":\"#Generic function for making a classification model and accessing the performance. \\\\n# From AnalyticsVidhya tutorial\\\\ndef classification_model(model, data, predictors, outcome):\\\\n  #Fit the model:\\\\n  model.fit(data[predictors],data[outcome])\\\\n  \\\\n  #Make predictions on training set:\\\\n  predictions = model.predict(data[predictors])\\\\n  \\\\n  #Print accuracy\\\\n  accuracy = metrics.accuracy_score(predictions,data[outcome])\\\\n  print(\\\\\"Accuracy : %s\\\\\" % \\\\\"{0:.3%}\\\\\".format(accuracy))\\\\n\\\\n  #Perform k-fold cross-validation with 5 folds\\\\n  kf = KFold(data.shape[0], n_folds=5)\\\\n  error = []\\\\n  for train, test in kf:\\\\n    # Filter training data\\\\n    train_predictors = (data[predictors].iloc[train,:])\\\\n    \\\\n    # The target we\\'re using to train the algorithm.\\\\n    train_target = data[outcome].iloc[train]\\\\n    \\\\n    # Training the algorithm using the predictors and target.\\\\n    model.fit(train_predictors, train_target)\\\\n    \\\\n    #Record error from each cross-validation run\\\\n    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\\\\n    \\\\n    print(\\\\\"Cross-Validation Score : %s\\\\\" % \\\\\"{0:.3%}\\\\\".format(np.mean(error)))\\\\n    \\\\n  #Fit the model again so that it can be refered outside the function:\\\\n  model.fit(data[predictors],data[outcome]) \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"33763a8f-3917-25c9-b581-d03be3078af2\"},\"source\":\"### Logistic Regression model\\\\n\\\\nLogistic regression is widely used for classification of discrete data. In this case we will use it for binary (1,0) classification.\\\\n\\\\nBased on the observations in the histogram plots, we can reasonably hypothesize that the cancer diagnosis depends on the mean cell radius, mean perimeter, mean area, mean compactness, mean concavity and mean concave points. We can then  perform a logistic regression analysis using those features as follows:\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"23d25895-a0ca-c355-899d-4a838e0d799d\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\',\\'perimeter_mean\\',\\'area_mean\\',\\'compactness_mean\\',\\'concave points_mean\\']\\\\noutcome_var=\\'diagnosis\\'\\\\nmodel=LogisticRegression()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"fcdbefb0-e8b7-990d-e016-5d6de31335a7\"},\"source\":\"The prediction accuracy is reasonable. \\\\nWhat happens if we use just one predictor? \\\\nUse the mean_radius:\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"44cdb278-4b12-5710-f616-db7ccde39775\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\']\\\\nmodel=LogisticRegression()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"24484a16-ccec-e32b-ec46-26f4e7572ad5\"},\"source\":\"This gives a similar prediction accuracy and a cross-validation score.\\\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6572777a-8c9c-1678-3651-65bd79577580\"},\"source\":\"The accuracy of the predictions are good but not great. The cross-validation scores are reasonable. \\\\nCan we do better with another model?\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"2a7ce79c-2557-d306-977c-c44adde27c6b\"},\"source\":\"### Decision Tree Model\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"36acf35e-296e-68fa-ab18-49967554af07\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\',\\'perimeter_mean\\',\\'area_mean\\',\\'compactness_mean\\',\\'concave points_mean\\']\\\\nmodel = DecisionTreeClassifier()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5d88c4cb-d2c7-ca71-5e00-786dd20480b3\"},\"source\":\"Here we are over-fitting the model probably due to the large number of predictors.\\\\nLet use a single predictor, the obvious one is the radius of the cell.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"20ef1197-b886-1517-1282-fe6dbeef15a4\"},\"outputs\":[],\"source\":\"predictor_var = [\\'radius_mean\\']\\\\nmodel = DecisionTreeClassifier()\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"936de076-96bd-82d4-25df-284744bc97b5\"},\"source\":\"The accuracy of the prediction is much much better here.  But does it depend on the predictor?\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"7f22bd99-d2e0-b7b8-c100-88a9a0f0c430\"},\"source\":\"Using a single predictor gives a 97% prediction accuracy for this model but the cross-validation score is not that great. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"40ca5482-3e1c-b8c1-2ac2-041897c9d160\"},\"source\":\"### Randome Forest\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"e6edf958-508b-f46c-74a5-f1d8f421900f\"},\"outputs\":[],\"source\":\"# Use all the features of the nucleus\\\\npredictor_var = features_mean\\\\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model, traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6e6fb3f9-6fea-171f-fd18-b29d475c140c\"},\"source\":\"Using all the features improves the prediction accuracy and the cross-validation score is great.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9e9bb834-8871-5d30-5832-0655b005f8e4\"},\"source\":\" An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cc9f7243-0ad7-5d47-9791-a0be8757ff1c\"},\"outputs\":[],\"source\":\"#Create a series with feature importances:\\\\nfeatimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\\\\nprint(featimp)\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"cb067a66-eea5-e2e3-ebc3-b18e832680e1\"},\"outputs\":[],\"source\":\"# Using top 5 features\\\\npredictor_var = [\\'concave points_mean\\',\\'area_mean\\',\\'radius_mean\\',\\'perimeter_mean\\',\\'concavity_mean\\',]\\\\nmodel = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model,traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"cf8789ef-3898-7c7d-7e46-7526880d71c6\"},\"source\":\"Using the top 5 features only changes the prediction accuracy a bit but I think we get a better result if we use all the predictors.\\\\n\\\\nWhat happens if we use a single predictor as before? Just check.\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"3a80b53f-3033-0d57-2d49-9089338b41d6\"},\"outputs\":[],\"source\":\"predictor_var =  [\\'radius_mean\\']\\\\nmodel = RandomForestClassifier(n_estimators=100)\\\\nclassification_model(model, traindf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"d28e57c8-4e19-1f69-f7e2-b11a3c392d84\"},\"source\":\"This gives a better prediction accuracy too but the cross-validation is not great.\\\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"48ad851d-8bb1-386d-30cb-675acf93d078\"},\"source\":\"## Using on the test data set\"},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"_cell_guid\":\"0666b20a-7359-2cf6-2792-08103d78afca\"},\"outputs\":[],\"source\":\"# Use all the features of the nucleus\\\\npredictor_var = features_mean\\\\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\\\\nclassification_model(model, testdf,predictor_var,outcome_var)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"4d63d451-380e-1dde-b3da-048afc63d019\"},\"source\":\"The prediction accuracy for the test data set using the above Random Forest model is 95%!\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0310c568-10fc-b19d-b2ee-c87a8f3f3280\"},\"source\":\"## Conclusion\\\\n\\\\nThe best model to be used for diagnosing breast cancer as found in this analysis is the Random Forest model with the top 5 predictors, \\'concave points_mean\\',\\'area_mean\\',\\'radius_mean\\',\\'perimeter_mean\\',\\'concavity_mean\\'. It gives a prediction accuracy of ~95% and a cross-validation score ~ 93% for the test data set.\\\\n\\\\n\\\\nI will see if I can improve this more by tweaking the model further and trying out other models in a later version of this analysis.\"}],\"metadata\":{\"_change_revision\":0,\"_is_fork\":false,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"file_extension\":\".py\",\"mimetype\":\"text/x-python\",\"name\":\"python\",\"nbconvert_exporter\":\"python\",\"pygments_lexer\":\"ipython3\",\"version\":\"3.5.2\"}},\"nbformat\":4,\"nbformat_minor\":0}',\n",
       " 'hasSource': True,\n",
       " 'language': 'python',\n",
       " 'hasLanguage': True,\n",
       " 'kernelType': 'notebook',\n",
       " 'hasKernelType': True,\n",
       " 'slug': 'breast-cancer-prediction',\n",
       " 'hasSlug': True}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['blob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'query': query\n",
      "'title': title\n",
      "'kernel_ref': kernel_ref\n"
     ]
    }
   ],
   "source": [
    "for i in ['query', 'title', 'kernel_ref']: \n",
    "    print(f\"'{i}': {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('notebooksearch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bae0182bf86e0b0e1f188bb3e25b1705b4b45c23a398ddc54779b24f1e501416"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

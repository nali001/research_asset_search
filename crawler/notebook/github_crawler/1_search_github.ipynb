{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97438549-07e2-43a5-b76f-780219c85a09",
   "metadata": {},
   "source": [
    "# Search notebooks on github\n",
    "We search the Github using `collected_queries` and `query_combinations`, to get the number of repos, and select queries that has 1-100 repos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f0b23-bef3-4c32-8403-82a9d8ed2f48",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "We use web scraping to extract the number of repos from Github, \n",
    "because PyGithub API returns outdated search results. \n",
    "\n",
    "But this is truly flawed, every 10 requests, the program has to be either re-run once or rate limit occurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f136766-7787-491f-8360-88485803ba52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdef12dc-b5d8-499f-b0db-4752ccd0ceee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "COLLECTED_QUERIES_FILE = '../../data/query/questions/collected_queries.txt'\n",
    "COLLECTED_QUERIES_RESULT_FILE = 'statistics/github_repos_collected_queries.csv'\n",
    "\n",
    "COLLECTED_QUERIES_COMBINATIONS_FILE = '../../data/query/questions/collected_queries_combinations.txt'\n",
    "COLLECTED_QUERIES_COMBINATIONS_RESULT_FILE = 'statistics/github_repos_combinations.csv'\n",
    "\n",
    "GITHUB_REPOS_PATH = \"github_repos\"\n",
    "REPO_LINKS_FILE = \"repo_links/collected_queries_repo_urls.txt\"\n",
    "FILTERED_QUERY_FILE = 'statistics/queries_repo_1_100.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9ee2f-2a1e-4fc8-9e39-d9e3387f5194",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get the number of repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2fde9e6-8583-400b-90db-4c11b4f22ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deduplicate_txt(file): \n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = list(set(lines))\n",
    "\n",
    "    with open(file, \"w\") as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a9d4ee1-5cd5-415e-a343-961f46e4205e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_row_exists(file_path, query):\n",
    "    # Open the CSV file for reading\n",
    "    with open(file_path, 'r', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        \n",
    "        # Check if the query exists in the file\n",
    "        for row in reader:\n",
    "            if row[0] == query:\n",
    "                # print(row)\n",
    "                # If repo_count is empty, return False\n",
    "                if row[1] == '':\n",
    "                    return False\n",
    "                # Otherwise, return True\n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "        # If the query does not exist in the file, return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def update_csv(file_path, query, repo_count):\n",
    "    # Check if the CSV file exists\n",
    "    try:\n",
    "        with open(file_path, 'r', newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            rows = [row for row in reader]\n",
    "    except FileNotFoundError:\n",
    "        # Create a new CSV file if it doesn't exist\n",
    "        with open(file_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['query', 'repo_count'])\n",
    "        rows = []\n",
    "    \n",
    "    # Check if the query already exists in the CSV file\n",
    "    for row in rows:\n",
    "        if row[0] == query:\n",
    "            # Update the repo_count if it's not empty\n",
    "            if repo_count is not None:\n",
    "                row[1] = repo_count\n",
    "                with open(file_path, 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerows(rows)\n",
    "            return row\n",
    "    \n",
    "    # Add a new row to the CSV file if the query doesn't exist\n",
    "    if repo_count is not None:\n",
    "        row = [query, repo_count]\n",
    "        with open(file_path, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(row)\n",
    "        return row\n",
    "    \n",
    "    # Return None if the repo_count is None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2158ba3c-38b2-4ce1-b2f8-9e3b6c4714d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to extract the number of repositories from the GitHub search results page\n",
    "def extract_repo_count(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # No result: \"We couldn't find any repositories matching...\"\n",
    "    for h3_tag in soup.find_all('h3'):\n",
    "        # Extract the contents of the current <h3> tag\n",
    "        h3_contents = h3_tag.get_text()\n",
    "        # print(h3_contents)\n",
    "        if \"find any repositories matching\" in h3_contents:\n",
    "            return 0\n",
    "    \n",
    "    results_count = soup.find('div', {'class': 'd-flex flex-column flex-md-row flex-justify-between border-bottom pb-3 position-relative'})\n",
    "    # Result 1: \"XXX repository results\"\n",
    "    try: \n",
    "    # Check the number of repositories\n",
    "        string = results_count.find('h3').get_text().strip()\n",
    "        pattern = r'\\d{1,3}(?:,\\d{3})*\\s+repository results'  # This regular expression matches any number with or without commas, followed by \" repository results\"\n",
    "    \n",
    "    # Result 2: \"Showing XXX available repository results\"\n",
    "    except: \n",
    "        string = results_count.find(\"span\", {\"class\": \"v-align-middle\"}).get_text().strip()\n",
    "        string = re.sub(r'\\s+', ' ', string)\n",
    "        pattern = r'Showing \\d{1,3}(?:,\\d{3})*\\s+available repository results'\n",
    "        \n",
    "    match = re.search(pattern, string)\n",
    "    \n",
    "    if match:\n",
    "        count = int(match.group(0).split()[0].replace(',', ''))\n",
    "        return count\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Helper function to get the number of repositories for a given query\n",
    "def get_repo_count(query):\n",
    "    SEARCH_QUERY = f\"{query} language:\\\"Jupyter Notebook\\\"\"\n",
    "    SEARCH_SORT = \"stars\"\n",
    "    SEARCH_ORDER = \"desc\"\n",
    "    url = f\"https://github.com/search?q={SEARCH_QUERY}&type=Repositories&s={SEARCH_SORT}&o={SEARCH_ORDER}\"\n",
    "    \n",
    "    time.sleep(2)\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        # print(type(response.content))\n",
    "        try: \n",
    "            count = extract_repo_count(response.content)\n",
    "            return count\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # return get_repo_count(query)\n",
    "            return -1\n",
    "    elif response.status_code == 429:\n",
    "        # If we get a 429 error, wait for the recommended number of seconds and retry the request\n",
    "        retry_after = response.headers.get('Retry-After')\n",
    "        if retry_after:\n",
    "            retry_after_secs = int(retry_after)\n",
    "            print(f'Rate limit exceeded. Waiting for {retry_after_secs} seconds before retrying...')\n",
    "            time.sleep(retry_after_secs)\n",
    "            return get_repo_count(query)\n",
    "    else:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36db2a7b-4feb-46fb-9224-2a3f1b25ff86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Post processing'\n",
    "get_repo_count(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ca95e75-d6e5-41d2-a056-b4459d65c4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ../../data/query/questions/collected_queries.txt ---\n",
      "--- ../../data/query/questions/collected_queries_combinations.txt ---\n",
      "['Attention-based multiple instance learning pytorch', -1]\n",
      "['Attention-based multiple instance learning whole slide image diagnosis', 0]\n",
      "['edge computing', 102]\n",
      "['meta learning', 463]\n"
     ]
    }
   ],
   "source": [
    "# ------ Get the number of repos ------\n",
    "# Check if the file exists\n",
    "files = [(COLLECTED_QUERIES_FILE, COLLECTED_QUERIES_RESULT_FILE), \n",
    "                          (COLLECTED_QUERIES_COMBINATIONS_FILE, COLLECTED_QUERIES_COMBINATIONS_RESULT_FILE)]\n",
    "for file in files: \n",
    "    input_file = file[0]\n",
    "    output_file = file[1]\n",
    "    \n",
    "    print(f\"--- {input_file} ---\")\n",
    "    if not os.path.isfile(output_file):\n",
    "        # If it doesn't exist, create it with the header row\n",
    "        with open(RESULT_PATH, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['query', 'repo_count'])\n",
    "\n",
    "    # Read the queries\n",
    "    queries = []\n",
    "    with open(input_file) as f:\n",
    "        for line in f:\n",
    "            query = line.strip()\n",
    "            queries.append(query)\n",
    "\n",
    "    # Get the repository counts for each query and store the results in a CSV file\n",
    "    for query in queries:\n",
    "        # print(query)\n",
    "        # First check if the query is there but repo_count is empty\n",
    "        if not check_row_exists(output_file, query): \n",
    "            count = None\n",
    "            for i in range(10): \n",
    "                count = get_repo_count(query)\n",
    "                if count is not None:\n",
    "                    break\n",
    "                # Check if this is the last iteration\n",
    "                if i == 9:\n",
    "                    print(\"Still returned None after 10 loops. Exiting program.\")\n",
    "                    sys.exit()\n",
    "            print([query, count])\n",
    "            update_csv(output_file, query, count)\n",
    "        else: \n",
    "            continue\n",
    "\n",
    "# Deduplicate the csv file\n",
    "# deduplicate_csv(RESULT_PATH)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d50365-d8a1-4ae7-83bd-55a2153a1930",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the queries with # repos [1-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fc7c3db-4cca-4fd0-95fc-4a10ae8c6230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_queries(file_path, output_file):\n",
    "    # Open the output file for writing\n",
    "    with open(output_file, 'a') as outfile:\n",
    "        # Read the CSV file and extract the queries\n",
    "        with open(file_path, 'r', newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                repo_count = row['repo_count']\n",
    "                if repo_count.isdigit() and 1 <= int(repo_count) <= 100:\n",
    "                    query = row['query']\n",
    "                    outfile.write(query + '\\n')\n",
    "    \n",
    "    deduplicate_txt(output_file)\n",
    "\n",
    "extract_queries(COLLECTED_QUERIES_RESULT_FILE, FILTERED_QUERY_FILE)\n",
    "extract_queries(COLLECTED_QUERIES_COMBINATIONS_RESULT_FILE, FILTERED_QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a7b8e-53cb-44ad-8c89-889379d5014b",
   "metadata": {},
   "source": [
    "### Get the repo links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663b0b89-3d7f-4051-bc6c-7d931364346f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_record_exists(file_path): \n",
    "    if not os.path.isfile(file_path):\n",
    "        # The file doesn't exist\n",
    "        return False\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        if len(data[\"repo_urls\"]) == 0: \n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57eb0b54-3d9a-4cfa-9ab5-f83844f4ddf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_repo_links(html): \n",
    "    repo_urls = []\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find all repository links in the HTML using the \"v-align-middle\" CSS class\n",
    "    repo_links = soup.find_all(\"a\", class_=\"v-align-middle\")\n",
    "    # Extract the repository URLs from the links and print them\n",
    "    for link in repo_links:\n",
    "        repo_url = \"https://github.com\" + link[\"href\"]\n",
    "        repo_urls.append(repo_url)\n",
    "        # print(repo_url)      \n",
    "    return repo_urls\n",
    "        \n",
    "def get_repo_links(query):\n",
    "    SEARCH_QUERY = f\"{query} language:\\\"Jupyter Notebook\\\"\"\n",
    "    SEARCH_SORT = \"stars\"\n",
    "    SEARCH_ORDER = \"desc\"    \n",
    "    # Initialize an empty list to store the repository URLs\n",
    "    repo_urls = []\n",
    "    # Loop over the top 3 pages of search results\n",
    "    for page in range(1, 4):\n",
    "        # Construct the search URL for the current page\n",
    "        search_url = f\"https://github.com/search?q={SEARCH_QUERY}&type=Repositories&s={SEARCH_SORT}&o={SEARCH_ORDER}&p={page}\"\n",
    "\n",
    "        # Send a GET request to the search URL and parse the HTML response using BeautifulSoup\n",
    "        time.sleep(2)\n",
    "        response = requests.get(search_url)\n",
    "        if response.ok:\n",
    "            # print(type(response.content))\n",
    "            try: \n",
    "                repo_links = extract_repo_links(response.content)\n",
    "                repo_urls = repo_urls + repo_links\n",
    "                if len(repo_links) < 10: \n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                # return get_repo_count(query)\n",
    "                return -1\n",
    "        elif response.status_code == 429:\n",
    "            # If we get a 429 error, wait for the recommended number of seconds and retry the request\n",
    "            retry_after = response.headers.get('Retry-After')\n",
    "            if retry_after:\n",
    "                retry_after_secs = int(retry_after)\n",
    "                print(f'Rate limit exceeded. Waiting for {retry_after_secs} seconds before retrying...')\n",
    "                time.sleep(retry_after_secs)\n",
    "                return get_repo_links(query)\n",
    "        else:\n",
    "            return -1\n",
    "    return repo_urls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06e0c23-7d35-4558-ac47-e3fcb5b7508a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/millingab/multiview-stereo',\n",
       " 'https://github.com/jacobastern/graph-construction-semi-supervised',\n",
       " 'https://github.com/y-richie-y/badgraphs',\n",
       " 'https://github.com/YEZIQM/Node2Vec_Subject_similarity',\n",
       " 'https://github.com/zzp1012/neo4j-enterprise-KG',\n",
       " 'https://github.com/hajarmerbouh/Cybersecurity-Knowledge-graph',\n",
       " 'https://github.com/yuhaozhang94/changi-airport-taxiway-planning',\n",
       " 'https://github.com/jiayiwus1x/physicist-net',\n",
       " 'https://github.com/cadovid/hn-data-pipeline',\n",
       " 'https://github.com/maxwellmckee/Convex_Clusters',\n",
       " 'https://github.com/FadedIllusions/AAE_Notebook_013_ProbabilisticRoadmap',\n",
       " 'https://github.com/arnavanand7/Market-Value-of-Footballers',\n",
       " 'https://github.com/think-high/Text_to_Knowledge_Graph',\n",
       " 'https://github.com/raminetinati/Knowledge-Graph',\n",
       " 'https://github.com/nkasmanoff/nasa-eo-knowledge-graph',\n",
       " 'https://github.com/DavidBraslow/Our-Class-Graph',\n",
       " 'https://github.com/baller609/Directed-Acyclic-Graph',\n",
       " 'https://github.com/mehdirostami/TauStarUndirectedGraph',\n",
       " 'https://github.com/NNSatyaKarthik/DeBruijnGraph',\n",
       " 'https://github.com/sherax139/Graph-Neural-Network-model',\n",
       " 'https://github.com/rohansheth17/probabilistic-graphical-models',\n",
       " 'https://github.com/YukiChen-yuxin/Design-and-implementation-of-a-task-based-Q-A-system-based-on-TCM-knowledge-graph',\n",
       " 'https://github.com/liamcarroll/music-recsys',\n",
       " 'https://github.com/2724170230/Optimization-Model-for-Emergency-Shelter-Location-Selection',\n",
       " 'https://github.com/unza-farhat/Jawan-Pakistan_final_project_of_data_science-',\n",
       " 'https://github.com/abose17/HPCGCN',\n",
       " 'https://github.com/wesley-kayanan/gene-smd-plants-som',\n",
       " 'https://github.com/LindaCai5/Neural-Network-Project']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"construct the graph\"\n",
    "get_repo_links(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10ca521-3764-4ca7-9ed2-bf21a92ac249",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ Download the results in the first 3 pages ------\n",
    "# Read the queries\n",
    "queries = []\n",
    "with open(FILTERED_QUERY_FILE) as f:\n",
    "    for line in f:\n",
    "        query = line.strip()\n",
    "        queries.append(query)\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e4513e5-d7e6-4833-a0b4-edce80932286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the repository links for each query and store the results in a json file\n",
    "for query in queries:\n",
    "    filename = \"github_repos/\" + query.replace(\" \", \"_\").replace(\"/\", \"_\") + \".json\"\n",
    "    if not os.path.isfile(filename): \n",
    "        for i in range(10): \n",
    "            repo_urls = get_repo_links(query)\n",
    "            if len(repo_urls) > 0:\n",
    "                break\n",
    "            # Check if this is the last iteration\n",
    "            if i == 9:\n",
    "                print(\"Still returned None after 10 loops. Exiting program.\")\n",
    "                sys.exit()\n",
    "        # Define the output dictionary with the query and repo_links fields\n",
    "        output_dict = {\n",
    "            \"query\": query,\n",
    "            \"repo_urls\": repo_urls\n",
    "        }\n",
    "        print(f\"{query}: {len(repo_urls)}\")\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(output_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1551d514-76b3-4292-ba96-aa422895d9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of repos: 975\n",
      "Number of unique repos: 915\n"
     ]
    }
   ],
   "source": [
    "# Extract repo links\n",
    "repo_urls = []\n",
    "\n",
    "# List all the files in the directory\n",
    "files = os.listdir(GITHUB_REPOS_PATH)\n",
    "\n",
    "# Iterate over the files and read all the JSON files\n",
    "for file in files:\n",
    "    if file.endswith(\".json\"):\n",
    "        file_path = os.path.join(GITHUB_REPOS_PATH, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            repo_urls.extend(data[\"repo_urls\"])\n",
    "\n",
    "print(f\"Number of repos: {len(repo_urls)}\")\n",
    "# Deduplicate the repo URLs\n",
    "repo_urls = list(set(repo_urls))\n",
    "\n",
    "print(f\"Number of unique repos: {len(repo_urls)}\")\n",
    "\n",
    "# Write all the repo URLs to a text file\n",
    "with open(REPO_LINKS_FILE, \"w\") as f:\n",
    "    for url in repo_urls:\n",
    "        f.write(url + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c4305-31a4-42a1-ac24-be79a6b1c2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

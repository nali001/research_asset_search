{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "393d4db4-6a03-4975-bdcf-3c22ae8964ea",
   "metadata": {},
   "source": [
    "## Download notebooks from Github\n",
    "Given a Github repo link, we download the Jupyter notebooks from the repo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ed47815-9753-476b-8a96-99749fd95de1",
   "metadata": {},
   "source": [
    "## Use PyGithub\n",
    "+ We use PyGithub API to traverse a repository and download .ipynb files from the repo. \n",
    "+ `id = hashlib.sha256(html_url)` is used to generate ID for notebook\n",
    "+ We use a `notebook_metadata` to record the id and some other metadata for the notebooks\n",
    "+ We use `repo_download_log` to keep reack of the repos being downloaded, making it possible to consume downloading and skip the notebook already exists. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb63eac-bee8-4464-ad17-54d1f115ae9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from github import Github\n",
    "from github.GithubException import RateLimitExceededException\n",
    "import hashlib\n",
    "import csv\n",
    "import datetime\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b64740f-88e6-484a-a970-28ea1da9aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL_FILE_COLLECTED_QUERIES = 'repo_links/true_relevant.csv'\n",
    "# METADATA_LOG_JUPYTER_WIKI = 'download_logs/notebook_metadata_jupyter_wiki.csv'\n",
    "# METADATA_LOG_COLLECTED_QUERIES = 'download_logs/notebook_metadata_collected_queries.csv'\n",
    "REPO_DOWNLOAD_LOG = 'download_logs/repo_relevant_log.csv'\n",
    "NOTEBOOK_CONTENT_PATH = 'Github/relevant_notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8def199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NB_357d2231c5e32a1982a5d57ed26cf860e1f642789ab727f565f2d365019d7b73'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_url = 'https://github.com/eth-cscs/PythonHPC/blob/master/numba-cuda/3.memory-optimization.ipynb'\n",
    "'NB_'+hashlib.sha256(html_url.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07bb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_csv(file_path, row):\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187f1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_record_in_csv(file_path, search_dict, search_field):\n",
    "    ''' Check the record in csv file'''\n",
    "    # open the CSV file and read it into a list of dictionaries\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        data = [row for row in reader]\n",
    "\n",
    "    # search for a record with a specific value in a field\n",
    "    found = False\n",
    "    for record in data:\n",
    "        if record[search_field] == search_dict[search_field]:\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if found:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127f460f-bec1-4cc8-a4f9-075a8c7de490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Traverse the Github repo and download notebooks\n",
    "def traverse_contents(contents, dst_dir):\n",
    "    for content_file in contents:\n",
    "        if content_file.type == 'dir':\n",
    "            traverse_contents(repo.get_contents(content_file.path), dst_dir)\n",
    "        elif content_file.name.endswith('.ipynb'):\n",
    "            # Compute the SHA-256 hash of the html_url as the ID for the notebook\n",
    "            notebook_id = 'NB_'+hashlib.sha256(content_file.html_url.encode('utf-8')).hexdigest()\n",
    "            # print(notebook_id)\n",
    "            notebook_path = os.path.join(dst_dir, notebook_id+'.ipynb')\n",
    "            metadata_path = os.path.join(dst_dir, notebook_id+'.json')\n",
    "            # print(path)\n",
    "            \n",
    "            if not os.path.exists(metadata_path):\n",
    "                # Store metadata\n",
    "                ipynb_info = get_ipynb_info(content_file, notebook_id)\n",
    "                with open(metadata_path, 'w') as f:\n",
    "                    json.dump(ipynb_info, f)\n",
    "                    print(f\"Metadata: {ipynb_info['docid']}\")\n",
    "            \n",
    "            # Download notebook file only if there is no record in the metadata log\n",
    "            if not os.path.exists(notebook_path): \n",
    "                download_file(content_file, notebook_path)\n",
    "                print(f\"Notebook: {notebook_path}\")\n",
    "\n",
    "\n",
    "def download_file(content_file, path):\n",
    "    response = requests.get(content_file.download_url)\n",
    "    if response.status_code == 200:\n",
    "        # dirname = os.path.dirname(path)\n",
    "        # if dirname != '':\n",
    "        #     os.makedirs(dirname, exist_ok=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    elif response.status_code == 429:\n",
    "            # If we get a 429 error, wait for the recommended number of seconds and retry the request\n",
    "            retry_after = response.headers.get('Retry-After')\n",
    "            if retry_after:\n",
    "                retry_after_secs = int(retry_after)\n",
    "                print(f'Rate limit exceeded. Waiting for {retry_after_secs} seconds before retrying...')\n",
    "                time.sleep(retry_after_secs)\n",
    "                return download_file(content_file, path)\n",
    "    else: \n",
    "        return -1\n",
    "            \n",
    "def get_ipynb_info(content_file, notebook_id):  \n",
    "    # Extract the relevant metadata from the ContentFile object\n",
    "    ipynb_info = {\n",
    "        'docid': notebook_id,\n",
    "        'path': content_file.path,\n",
    "        'name': os.path.basename(content_file.path),\n",
    "        'html_url': content_file.html_url,\n",
    "        'url': content_file.url,\n",
    "        'size': content_file.size,\n",
    "        'sha': content_file.sha,\n",
    "        'git_url': content_file.git_url,\n",
    "        'download_url': content_file.download_url,\n",
    "        'type': content_file.type,\n",
    "        'encoding': content_file.encoding,\n",
    "        'last_modified': content_file.last_modified\n",
    "    }\n",
    "    return ipynb_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591f587d-e2ed-4633-87ff-a80fb583fd75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rate_limit_info(g): \n",
    "    # Get the rate limit information from the last response\n",
    "    rate_limit = g.get_rate_limit()\n",
    "    rate_remaining = rate_limit.core.remaining\n",
    "    rate_reset_utc = rate_limit.core.reset\n",
    "\n",
    "    # Convert UTC time to local time zone\n",
    "    local_tz = pytz.timezone('Europe/Amsterdam')  # Replace with your local time zone\n",
    "    rate_reset = rate_reset_utc.replace(tzinfo=pytz.utc).astimezone(local_tz)\n",
    "\n",
    "    # Print the rate limit information\n",
    "    print(f\"Rate limit: {rate_limit}\")\n",
    "    print(f\"Rate remaining: {rate_remaining}\")\n",
    "    print(f\"Rate reset time: {rate_reset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3169a1ab-94fe-44f4-945e-a2b93d00ca71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get github access token\n",
    "with open('github_token.json', 'r') as f:\n",
    "    # Load the JSON data into a dictionary\n",
    "    data = json.load(f)\n",
    "\n",
    "# Access the values of the 'user' and 'token' keys\n",
    "user = data['user']\n",
    "token = data['token']\n",
    "\n",
    "# Provide your access token or username and password\n",
    "g = Github(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05221f71-dd26-410a-a857-222dd2b7e01e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "# Get repo URLs\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(REPO_URL_FILE_COLLECTED_QUERIES)\n",
    "\n",
    "# with open(REPO_URL_FILE_COLLECTED_QUERIES, 'r') as f:\n",
    "#     urls = f.readlines()\n",
    "#     repo_urls = [url.strip() for url in urls]\n",
    "# print(repo_urls[:5])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa0bf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>repo_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EQ_1</td>\n",
       "      <td>https://github.com/the-pinbo/EC802-Low-Power-V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EQ_2</td>\n",
       "      <td>https://github.com/squidarth/demonstrating-con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EQ_3</td>\n",
       "      <td>https://github.com/CN-UPB/ml-for-resource-allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EQ_4</td>\n",
       "      <td>https://github.com/MKVarun/Bladder-cancer-segm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EQ_5</td>\n",
       "      <td>https://github.com/jafingerhut/p4-guide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid                                           repo_url\n",
       "0  EQ_1  https://github.com/the-pinbo/EC802-Low-Power-V...\n",
       "1  EQ_2  https://github.com/squidarth/demonstrating-con...\n",
       "2  EQ_3  https://github.com/CN-UPB/ml-for-resource-allo...\n",
       "3  EQ_4  https://github.com/MKVarun/Bladder-cancer-segm...\n",
       "4  EQ_5            https://github.com/jafingerhut/p4-guide"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d0b2af5-483f-42af-b450-72259618b468",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] the-pinbo/EC802-Low-Power-VLSI-Design\n",
      "[1] squidarth/demonstrating-congestion-control\n",
      "[2] CN-UPB/ml-for-resource-allocation\n",
      "[3] MKVarun/Bladder-cancer-segmentation\n",
      "[4] jafingerhut/p4-guide\n",
      "[5] mhamilton723/STEGO\n",
      "[6] NikolaiSviridov/Specular-Highlight-Removal\n",
      "[7] yli131/brainGRL\n",
      "[8] Adityagrao/Poincar-Embeddings-for-Learning-Hierarchical-Representations\n",
      "[9] fuzzland/audit_gpt\n",
      "[10] shahidzikria/ADD-Net\n",
      "[11] mahmoodlab/MCAT\n",
      "[12] affaniqbal/pl-data\n",
      "[13] tobias-fritz/Spectral-vis\n",
      "[14] BhadraNivedita/Scatterplot-in-R\n",
      "[15] NikolasMarkou/blind_image_denoising\n",
      "[16] Alerovere/Questionnaire\n",
      "[17] denocris/NLP-Workshop-MLMilan\n",
      "[18] optas/latent_3d_points\n",
      "[19] stes/saliency\n",
      "[20] BrijeshYadav001st/CPU-RAM-scratch\n",
      "[21] saravanan290702/Breast-Cancer-Prognostic-analysis\n",
      "[22] natandrade/Tutorial-Medical-Image-Registration\n",
      "[23] drewwilimitis/hyperbolic-learning\n",
      "[24] tjiagoM/spatio-temporal-brain\n",
      "[25] iamAdrianC/R_Data_Science_Survey\n",
      "[26] ziofil/live_plot\n",
      "[27] MbProg/Anomaly-detection-of-timeseries\n",
      "[28] seycho/WSI_Preprocess\n",
      "[29] AlessandraSozzi/Lucene-python\n",
      "[30] nikosgalanis/bsc-thesis\n",
      "[31] Rinkusoni2910/Prediction_Of_Dynamic_Cloud_Resources_Provisioning_for_Workflow\n",
      "[32] Shruti-codes236/RL-DDPG-Task-Scheduling\n",
      "[33] brfi3983/Manifold-Optimization\n",
      "[34] viktorp99/arucoMarker-detection\n",
      "[35] mirnanoukari/ArUco-Pose-Detection\n",
      "[36] mahmoodlab/MCAT\n",
      "[37] SiddhantG02/ECC188DeepLearning\n",
      "[38] podondra/data-preprocessing\n",
      "[39] woodyx218/Deep-Learning-with-GDP-Pytorch\n",
      "[40] samuelepapa/anomaly_detection_vae\n",
      "[41] shoaib555/Health-Insurance\n",
      "[42] Trotts/Siamese-Neural-Network-MNIST-Triplet-Loss\n",
      "[43] llSourcell/loss_functions_explained\n",
      "[44] wangz10/contrastive_loss\n",
      "[45] sebastian-hofstaetter/neural-ranking-kd\n",
      "[46] Freddavide/Sparse_retrieval_models\n",
      "[47] apolanco3225/Modelling-Time-Series-EHR\n",
      "[48] eth-cscs/PythonHPC\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the repository\n",
    "for index, row in data.iterrows():\n",
    "    parsed_url = urlparse(row['repo_url'])\n",
    "    path_parts = parsed_url.path.split('/')\n",
    "    user = path_parts[1]\n",
    "    repo = path_parts[2]\n",
    "\n",
    "    print(f'[{index}] {user}/{repo}')\n",
    "    record = {\n",
    "        'qid': row['qid'], \n",
    "        'repo_url': row['repo_url'], \n",
    "        'downloaded': True\n",
    "    }\n",
    "    if check_record_in_csv(REPO_DOWNLOAD_LOG, record, 'repo_url'): \n",
    "        # print(f'{repo_url} already downloaded!')\n",
    "        continue\n",
    "    else: \n",
    "        try: \n",
    "            repo = g.get_repo(f\"{user}/{repo}\")\n",
    "            contents = repo.get_contents('')\n",
    "            traverse_contents(contents, NOTEBOOK_CONTENT_PATH)\n",
    "            add_row_csv(REPO_DOWNLOAD_LOG, [row['qid'],row['repo_url'], True])\n",
    "        except RateLimitExceededException as e: \n",
    "            print(e)\n",
    "            get_rate_limit_info(g) \n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dce4582f-71e4-405f-95f7-d8c80685681a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .ipynb files in Github/relevant_notebooks: 170\n"
     ]
    }
   ],
   "source": [
    "# Count the number of notebooks\n",
    "def count_ipynb_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.ipynb'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# Example usage\n",
    "directory = NOTEBOOK_CONTENT_PATH\n",
    "count = count_ipynb_files(directory)\n",
    "print(f'Number of .ipynb files in {directory}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc35de2-96f9-4cb0-a93f-7cc112d35590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
